#!/usr/bin/env python
# -*- coding: utf-8 -*-

'''
Orchestrator: This Module orchestrates the Cisco VIM openstack install.

Requirements:
-------------
1. The module expects a roles/profiles (.yaml) file.
2. A hosts.ini file --> Which has the list of hosts categorized by profiles.

These files should have been pre-generated by the steps before.

Details:
--------
The Module reads the hosts.ini to identify the hosts to be orchestrated.
For each profile specific host list, it will perform all the tasks to setup
the nodes.
- It uses Ansible to perform basic orchestration of hosts.
- It uses other modules within orchestrator which setup the docker containers.
'''

import json
import os
import re
import subprocess
import sys
import time
import yaml

import simplejson
import utils.common as common
import utils.config_parser as config_parser
import utils.logger as logger
from nfvimon import nfvimon_helper as nfvimon_helper
import kubernetes.kube_control_cluster_deploy as kube_control_cluster_deploy
from threading import Thread
import Queue


DEFAULT_CFG_DIR = "openstack-configs"
DEFAULT_SETUP_FILE = "setup_data.yaml"
BACKUP_SETUP_FILE = ".backup_setup_data.yaml"
DEFAULTS_FILE = "defaults.yaml"
SECRETS_FILE = "secrets.yaml"
STAGING_SECRETS_FILE = "staging_secrets.yaml"
INVENTORY_FILE = "system_configs/_opestack_manifest.yaml"
OS_CONFIG_FILE = "openstack_config.yaml"
POLICY_EXEC_PLAN = "policy_exec_plan.yaml"
VAULT_FILE = "/opt/cisco/vault/key.yaml"
TASK_PATTERN = re.compile(r'.*TASK  *\[(\w+(?:[-\w]*\w)) *\ : *(.*) *]')

####################################################################
#Skip update of these containers. Some containers are not supported
#while some container updates happen in step 2
####################################################################

class OrchestratorStatus(object):
    '''
    Status
    '''
    OPER_STAGE = "Generating Inventory"
    STAGE_COUNT = 0


class Orchestrator(object):
    '''
    Orchestrator - orchestrate openstack deployment.
    '''
    def __init__(self):
        '''
        Initialize.
        '''
        ################################################
        # Set up logging
        ################################################
        self.loginst = logger.Logger(name=__name__)
        self.log = self.loginst.get_logger()
        self.name = __name__
        self.run_args = {}

        self.ymlhelper = config_parser.YamlHelper()
        if self.ymlhelper is None:
            self.log.error("Failed to parse YAML Files for configuration")
            sys.exit(1)

        self.log.debug("Orchestrator: Initialized")

    def set_oper_stage(self, msg):
        '''
        Set Operation stage status.
        '''
        OrchestratorStatus.OPER_STAGE = msg
        OrchestratorStatus.STAGE_COUNT += 1


def set_run_result(resobj, key, value):
    '''
    Set the run result
    '''
    robj = resobj

    # Never change status from FAIL to PASS.
    if key == "status":
        try:
            if resobj[key] == "PASS":
                robj[key] = value
        except KeyError:
            robj[key] = value
    else:
        robj[key] = value


def monitor_and_validate_ansible_output(orchestrator,
                                        nextline, result):
    '''
    Monitor ansible output and check for key error patterns to report
    error.
    '''
    # First check if this is a Task pattern.
    mobj = None
    mobj = TASK_PATTERN.match(nextline)
    if mobj:
        msg = mobj.group(1) + "-" + mobj.group(2)
        orchestrator.set_oper_stage(msg)
        set_run_result(result, 'status', 'PASS')
        return result

    _err_patterns = [r".*fatal:.*", r".*error.*", r".*failed=1.*"]
    for pattern in _err_patterns:
        mobj = re.match(pattern, nextline, flags=re.IGNORECASE)
        if mobj:
            msg = mobj.group(0)
            orchestrator.set_oper_stage(msg)
            set_run_result(result, 'status', 'FAIL')
            print logger.stringc(msg, 'red')
            return result

    set_run_result(result, 'status', 'PASS')
    return result


def print_ansible_usage():
    '''
    Print Usage/Help in case of failure.
    '''
    homedir = get_homedir()
    msg = "=" * 50 + "\n"
    msg += "  Cisco VIM Ansible Dynamic inventory generator" + "\n"
    msg += "=" * 50 + "\n"
    msg += "Validations Failed: " + "\n"
    msg += " 1. Ensure config data folder exists [%s/%s]" % (homedir,
                                                             DEFAULT_CFG_DIR)
    msg += "\n"
    msg += " 2. Ensure Setup Data file exist [%s/%s/%s]" % \
        (homedir, DEFAULT_CFG_DIR, DEFAULT_SETUP_FILE)

    msg += "\n"
    msg += " 3. Ensure User secrets file exist [%s/%s/%s]" % \
        (homedir, DEFAULT_CFG_DIR,
         SECRETS_FILE)
    msg += "\n"
    print msg


def get_homedir():
    '''
    Get the current username
    '''
    homedir = os.path.expanduser("~")
    return homedir


def load_policy_exec_plan(action, run_args_id):
    # (schoksey): Need to revisit the policy definition struct
    cfg_dir = os.path.join(get_homedir(), DEFAULT_CFG_DIR)
    exec_plan_file = os.path.join(cfg_dir, POLICY_EXEC_PLAN)
    try:
        parsed_config = config_parser.YamlHelper(user_input_file=exec_plan_file)
    except IOError:
        return None
    return parsed_config.get_exec_plan(action, run_args_id) \
        if parsed_config and run_args_id and \
           parsed_config.get_optimization_mode(action, run_args_id) \
        else None


def get_all_exec_playbooks(action, run_args_id):
    exec_plan_playbooks = []
    exec_plan = load_policy_exec_plan(action, run_args_id)
    if exec_plan:
        for group in exec_plan:
            for playbook in group['playbooks']:
                ansible_playbook = os.path.join(
                    os.path.dirname(common.get_absolute_path_for_file(__file__, "tmp")),
                    "playbooks", playbook['name'])
                exec_plan_playbooks.append(ansible_playbook)
    return exec_plan_playbooks


def get_openstack_update_service_manifest(update_file):
    '''
    Get Update manifest from update.yaml
    '''
    homedir = get_homedir()
    cfg_dir = os.path.join(homedir, DEFAULT_CFG_DIR)
    setup_file = os.path.join(cfg_dir, DEFAULT_SETUP_FILE)
    services_to_update = []
    cprop = common.cont_prop()
    non_orchestrator_services = cprop["BUILD_NODE_CONTAINERS"] + \
        cprop["NO_UPDATE_CONTAINERS"] + cprop["CEPH_CONTAINERS"] + \
        cprop["DEV_CONTAINERS"]
    parsed_userinput = config_parser.YamlHelper(user_input_file=setup_file)
    optional_services = parsed_userinput.\
        get_data_from_userinput_file(['OPTIONAL_SERVICE_LIST'])
    mechanism_drivers = parsed_userinput.\
        get_data_from_userinput_file(['MECHANISM_DRIVERS'])

    if optional_services is None:
        optional_services = []

    if nfvimon_helper.is_nfvimon_enabled():
        optional_services.append("ceilometer")

    with open(update_file) as f:
        services = yaml.safe_load(f)
        services_to_update.extend(services)

    if services_to_update:
        unsupported_service = list(services_to_update)
        for service in unsupported_service:
            if service in non_orchestrator_services:
                services_to_update.remove(service)

    opt_cont = cprop['OPTIONAL_CONTAINERS']
    for opt_serv in opt_cont.keys():
        if opt_serv in optional_services:
            continue
        if (opt_serv == 'gnocchi') and ('ceilometer' in optional_services):
            continue
        for serv in opt_cont[opt_serv]:
            try:
                services_to_update.remove(serv)
            except ValueError:
                pass

    filter_list = []
    if mechanism_drivers == "linuxbridge":
        filter_list = cprop['SKIP_LB_UPDATE']
    elif mechanism_drivers == "vts":
        filter_list = cprop['SKIP_VTS_UPDATE']
    elif mechanism_drivers == "vpp":
        filter_list = cprop['SKIP_VPP_UPDATE']
    else:
        # Assume default of OVS containers
        filter_list = cprop['SKIP_OVS_UPDATE']

    for serv in filter_list:
        try:
            services_to_update.remove(serv)
        except ValueError:
            pass

    if not parsed_userinput.create_sriov() \
            and "neutron-sriov" in services_to_update:
        services_to_update.remove("neutron-sriov")

    if not parsed_userinput.check_section_exists('SWIFTSTACK')\
            and "cinder-backup" in services_to_update:
        services_to_update.remove("cinder-backup")

    return services_to_update

def build_ansible_cmd(orchestrator, setup_file, defaults_file, \
                      secrets_file, docker_file, os_config_file, \
                      ansible_playbook):
    '''
    Build the ansible cmd string based on the setup files,
    and arguments passed to the orchestrator module.
    '''
    parsed_userinput = config_parser.YamlHelper(user_input_file=setup_file)

    # Userinput files.
    setup_file = "@" + setup_file
    defaults_file = "@" + defaults_file
    secrets_file = "@" + secrets_file
    docker_file = "@" + docker_file
    os_config_file = "@" + os_config_file

    # Generate Ansible command.
    ansible_cmd = ["ansible-playbook", ansible_playbook,
                   "-e", setup_file, "-e", defaults_file,
                   "-e", secrets_file, "-e", docker_file,
                   "-e", os_config_file]

    # Set additional arguments.
    ansible_options = ""
    pod_type = parsed_userinput.get_pod_type() or None
    tags = orchestrator.run_args.get('tags', None)
    skip_tags = orchestrator.run_args.get('skip_tags', None)
    controller = orchestrator.run_args.get('replace_controller', None)
    add_computes = orchestrator.run_args.get('add_computes', None)
    remove_computes = orchestrator.run_args.get('remove_computes', None)
    reconfig_computes = orchestrator.run_args.get('reconfig_computes', None)
    aim_computes = orchestrator.run_args.get('aim_computes', None)
    removed_controller = orchestrator.run_args.get('removed_controller', None)
    force = orchestrator.run_args.get('force', None)
    osds = orchestrator.run_args.get('add_osds', None)
    remove_osd = orchestrator.run_args.get('remove_osd', None)
    action = orchestrator.run_args.get('action', None)
    execute = orchestrator.run_args.get('execute', None)
    osdinfo = orchestrator.run_args.get('osdinfo', None)
    vault_reconfigure = orchestrator.run_args.get('vault_reconfigure', None)
    playbook = "openstack-install.yaml"
    fernet = orchestrator.run_args.get('fernet_rotation', None)
    run_disk_checks = orchestrator.run_args.get('run_disk_checks', None)
    update_scope_file = os.path.join(get_homedir(), DEFAULT_CFG_DIR) + \
        "/update_scope.yaml"
    update_file = os.path.join(get_homedir(), DEFAULT_CFG_DIR) + "/update.yaml"
    upgrade_file = os.path.join(get_homedir(), DEFAULT_CFG_DIR) + "/upgrade.yaml"
    reconfigure_file = os.path.join(get_homedir(), DEFAULT_CFG_DIR) + \
        "/openstack_config.yaml"

    vault_config = parsed_userinput.get_vault_info()
    if vault_config and vault_config['enabled'] and os.path.exists(VAULT_FILE):
        with open(VAULT_FILE, 'r') as f:
            data = yaml.safe_load(f.read())
            token = data['root_token']
        ansible_option = ansible_options + "-e TOKEN=%s" % token
        ansible_cmd.append(ansible_option)

    replace_controller_ip = None
    if controller:
        yaml_file = common.get_cobbler_data_file_path()
        cobbler_file = config_parser.YamlHelper(user_input_file=yaml_file)
        cobbler_file_dict = cobbler_file.create_parsed_yaml(yaml_file)
        controller_ipaddr = (cobbler_file_dict[controller[0]]['bonds']
                             ['management']['ipaddress'])
        replace_controller_ip = controller_ipaddr
        if not execute:
            ansible_option = ansible_options + "-e server=:&%s" % controller_ipaddr
            ansible_cmd.append(ansible_option)
            ansible_option = ansible_options + "--extra-vars=" + \
                             simplejson.dumps({"replace_controller_ip": replace_controller_ip,
                                               "replace_controller_host": controller[0]},
                                              separators=(',', ':'),
                                              ensure_ascii=False)
            ansible_cmd.append(ansible_option)

    if add_computes:
        yaml_file = common.get_cobbler_data_file_path()
        cobbler_file = config_parser.YamlHelper(user_input_file=yaml_file)
        cobbler_file_dict = cobbler_file.create_parsed_yaml(yaml_file)
        compute_nodes = []
        for compute in add_computes:
            compute_ipaddr = (cobbler_file_dict[compute]['bonds']
                              ['management']['ipaddress'])
            compute_nodes.append(str(compute_ipaddr))
        ansible_option = ansible_options + "--extra-vars=" + \
            simplejson.dumps({"COMPUTE": compute_nodes},
                             separators=(',', ':'), ensure_ascii=False)
        if not tags:
            tag_info = ['base']
            if parsed_userinput.is_ceilometer_enabled():
                tag_info.append('ceilometer')
            if nfvimon_helper.is_nfvimon_enabled():
                tag_info.append('ceilometer')
                tag_info.append('nfvimon')
            mechanism_drivers = parsed_userinput.\
              get_data_from_userinput_file(['MECHANISM_DRIVERS'])

            tag_info.append(mechanism_drivers)

            if parsed_userinput.create_sriov():
                tag_info.append("neutron-sriov")

            tags = ",".join(tag_info)

        ansible_cmd.append(ansible_option)

    if remove_computes:
        yaml_file = common.get_cobbler_data_file_path()
        cobbler_file = config_parser.YamlHelper(user_input_file=yaml_file)
        cobbler_file_dict = cobbler_file.create_parsed_yaml(yaml_file)
        if force:
            ansible_option = ansible_options + "-e FORCE=True"
            ansible_cmd.append(ansible_option)
        compute_ips = []
        for compute in remove_computes:
            if compute in cobbler_file_dict:
                compute_ipaddr = (cobbler_file_dict[compute]['bonds']
                                  ['management']['ipaddress'])
                compute_ips.append(str(compute_ipaddr))
        ansible_option = "--extra-vars=" + \
                         simplejson.dumps({"compute_ips": compute_ips},
                                          separators=(',', ':'),
                                          ensure_ascii=False)
        ansible_cmd.append(ansible_option)

        ansible_option = ansible_options + "--extra-vars=" + \
            simplejson.dumps({"COMPUTE": remove_computes},
                             separators=(',', ':'), ensure_ascii=False)
        ansible_cmd.append(ansible_option)

    if aim_computes:
        ansible_option = "--extra-vars=" + \
                         simplejson.dumps({"aim_computes": aim_computes},
                                          separators=(',', ':'),
                                          ensure_ascii=False)
        ansible_cmd.append(ansible_option)

    if removed_controller:
        # removed_controller_ip='15.0.0.6'
        yaml_file = common.get_cobbler_data_file_path()
        cobbler_file = config_parser.YamlHelper(user_input_file=yaml_file)
        cobbler_file_dict = cobbler_file.create_parsed_yaml(yaml_file)
        removed_controller_ip = (cobbler_file_dict[removed_controller]['bonds']
                             ['management']['ipaddress'])
        ansible_option = "-e removed_controller_ip=%s" % removed_controller_ip
        ansible_cmd.append(ansible_option)

    if remove_osd:
        yaml_file = common.get_cobbler_data_file_path()
        cobbler_file = config_parser.YamlHelper(user_input_file=yaml_file)
        cobbler_file_dict = cobbler_file.create_parsed_yaml(yaml_file)
        osd_ips = []
        if remove_osd in cobbler_file_dict:
            osd_ip = (cobbler_file_dict[remove_osd]['bonds']
                      ['management']['ipaddress'])
            osd_ips.append(str(osd_ip))
        ansible_option = "--extra-vars=" + \
                         simplejson.dumps({"osd_ips": osd_ips},
                                          separators=(',', ':'),
                                          ensure_ascii=False)
        ansible_cmd.append(ansible_option)

        ansible_option = ansible_options + "--extra-vars=" + \
            simplejson.dumps({"OSD_HOST": remove_osd},
                             separators=(',', ':'), ensure_ascii=False)
        ansible_cmd.append(ansible_option)

    if osds:
        yaml_file = common.get_cobbler_data_file_path()
        cobbler_file = config_parser.YamlHelper(user_input_file=yaml_file)
        cobbler_file_dict = cobbler_file.create_parsed_yaml(yaml_file)
        osd_nodes = []
        for osd in osds:
            mgmt_stanza = cobbler_file_dict[osd]['bonds']['management']
            if mgmt_stanza.get('ipaddress', None):
                osd_ipaddr = mgmt_stanza['ipaddress']
                osd_nodes.append(str(osd_ipaddr))

        ansible_option = ansible_options + "--extra-vars=" + \
            simplejson.dumps({"OSD": osd_nodes},
                             separators=(',', ':'), ensure_ascii=False)

        if not tags:
            tag_info = ['base']
        else:
            tag_info = tags.split()
        if nfvimon_helper.is_nfvimon_enabled():
            tag_info.append('nfvimon')
        tags = ",".join(tag_info)
        ansible_cmd.append(ansible_option)

    if execute:
        ansible_option = ansible_options + "-eexecute=" + execute
        ansible_cmd.append(ansible_option)
        if replace_controller_ip:
            ansible_option =  \
                ansible_options + "-ereplace_controller_ip=" + replace_controller_ip
            ansible_cmd.append(ansible_option)

    if osdinfo:
        ansible_option = ansible_options + "-eosdinfo=" + osdinfo
        ansible_cmd.append(ansible_option)

    if reconfig_computes:
        yaml_file = common.get_cobbler_data_file_path()
        cobbler_file = config_parser.YamlHelper(user_input_file=yaml_file)
        cobbler_file_dict = cobbler_file.create_parsed_yaml(yaml_file)
        compute_nodes = []
        for compute in reconfig_computes:
            compute_ipaddr = (cobbler_file_dict[compute]['bonds']
                              ['management']['ipaddress'])
            compute_nodes.append(str(compute_ipaddr))
        ansible_option = "--extra-vars=" + \
                         simplejson.dumps({"RECONFIG_COMPUTES": compute_nodes},
                                          separators=(',', ':'), ensure_ascii=False)
        ansible_cmd.append(ansible_option)

    if action == "upgrade":
        ansible_option = ansible_options + \
            "-e" + "@" + upgrade_file
        ansible_cmd.append(ansible_option)
        ansible_option = ansible_options + "-eACTION=upgrade"
        ansible_cmd.append(ansible_option)
        if os.path.exists("/opt/cisco/skip_upgrade"):
            ansible_option = ansible_options + "-eSKIP_UPGRADE=True"
            ansible_cmd.append(ansible_option)
        tag_info = ['base']
        mechanism_drivers = parsed_userinput.\
          get_data_from_userinput_file(['MECHANISM_DRIVERS'])

        optional_services = parsed_userinput.\
          get_data_from_userinput_file(['OPTIONAL_SERVICE_LIST'])
        tag_info.extend(optional_services) if optional_services is not None else []
        # Add the right mechanism driver tag
        tag_info.append(mechanism_drivers)

        if parsed_userinput.create_sriov():
            tag_info.append("neutron-sriov")

        if parsed_userinput.check_section_exists('SWIFTSTACK'):
            tag_info.append("cinder-backup")

        if nfvimon_helper.is_nfvimon_enabled():
            tag_info.append('ceilometer')
            tag_info.append('nfvimon')
        if pod_type and pod_type == 'ceph':
            tag_info = ['central-ceph']
        tag = ",".join(tag_info)
        orchestrator.log.debug("Add tags %s to ANSIBLE CMD", tag)
        ansible_option = ansible_options + "--tags=%s" % tag
        ansible_cmd.append(ansible_option)

    if action == "reconfigure":
        ansible_option = ansible_options + \
            "-e" + "@" + reconfigure_file
        ansible_cmd.append(ansible_option)
        ansible_option = ansible_options + "-eACTION=reconfigure"
        ansible_cmd.append(ansible_option)
        if pod_type and  pod_type == 'ceph':
            tag_info = ['central-ceph']
        else:
            tag_info = ['base']
            mechanism_drivers = parsed_userinput.\
              get_data_from_userinput_file(['MECHANISM_DRIVERS'])

            optional_services = parsed_userinput.\
              get_data_from_userinput_file(['OPTIONAL_SERVICE_LIST'])
            tag_info.extend(optional_services) if optional_services is not None else []
            # Add the right mechanism driver tag
            tag_info.append(mechanism_drivers)

            if parsed_userinput.create_sriov():
                tag_info.append("neutron-sriov")

            if parsed_userinput.check_section_exists('SWIFTSTACK'):
                tag_info.append("cinder-backup")

            nfvimon_enabled = False
            if nfvimon_helper.is_nfvimon_enabled():
                tag_info.append('ceilometer')
                tag_info.append('nfvimon')
                nfvimon_enabled = True

            if nfvimon_helper.needs_nfvimon_removal():
                orchestrator.log.debug("Needs to remove nfvimon configuration")
                tag_info.append("remove-nfvimon")
            elif nfvimon_enabled:
                orchestrator.log.debug("Needs to reconfigure with nfvimon configured")
                ansible_option = ansible_options + "--skip-tags=remove-nfvimon"
                ansible_cmd.append(ansible_option)

        tag = ",".join(tag_info)
        orchestrator.log.debug("Add tags %s to ANSIBLE CMD", tag)
        ansible_option = ansible_options + "--tags=%s" % tag
        ansible_cmd.append(ansible_option)

    if action == "regenerate":
        ansible_option = ansible_options + "-eACTION=regenerate"
        ansible_cmd.append(ansible_option)
        ssh_key = orchestrator.run_args.get('nfvimon_ssh_key', None)
        if ssh_key:
            ansible_option = ansible_options + "-enfvimon_ssh_key=%s" % ssh_key
            ansible_cmd.append(ansible_option)

    if action == "update":
        ansible_option = ansible_options + \
            "-e" + "@" + update_file
        ansible_cmd.append(ansible_option)
        ansible_option = ansible_options + "-eACTION=update"
        ansible_cmd.append(ansible_option)
        services = get_openstack_update_service_manifest(update_scope_file)
        pod_tag = 'base'
        if pod_type and pod_type == 'ceph':
            pod_tag = 'central-ceph'
        services.extend([pod_tag])
        if len(parsed_userinput.get_server_list(role="block_storage")) != 0:
            services.extend(['update_storage'])
        ansible_option = ansible_options + "--tags=%s" % (",".join(services))
        ansible_cmd.append(ansible_option)

    if action == "commit":
        ansible_option = ansible_options + \
            "-e" + "@" + update_file
        ansible_cmd.append(ansible_option)
        ansible_option = ansible_options + "-eACTION=commit"
        ansible_cmd.append(ansible_option)
        services = get_openstack_update_service_manifest(update_scope_file)
        services.append('commit')
        ansible_option = ansible_options + "--tags=%s" % (",".join(services))
        ansible_cmd.append(ansible_option)

    if action == "rollback":
        ansible_option = ansible_options + \
            "-e" + "@" + update_file
        ansible_cmd.append(ansible_option)
        ansible_option = ansible_options + "-eACTION=rollback"
        ansible_cmd.append(ansible_option)
        services = get_openstack_update_service_manifest(update_scope_file)
        services.append('rollback')
        ansible_option = ansible_options + "--tags=%s" % (",".join(services))
        ansible_cmd.append(ansible_option)

    if fernet:
        ansible_option = ansible_options + "-eFERNET_ROTATION='%s'" % fernet
        orchestrator.log.debug("Fernet rotation: %s", ansible_option)
        ansible_cmd.append(ansible_option)

    if action == "check_fernet_keys":
        ansible_option = ansible_options + "-eFERNET_RECOVERY=check"
        ansible_cmd.append(ansible_option)

    if action == "resync_fernet_keys":
        ansible_option = ansible_options + "-eFERNET_RECOVERY=resync"
        ansible_cmd.append(ansible_option)

    if "cluster-recovery" in ansible_playbook:
        ansible_option = ansible_options + "-eCLUSTER_RECOVERY=true"
        ansible_cmd.append(ansible_option)

    if run_disk_checks:
        ansible_option = ansible_options + "-eRUN_DISK_CHECK={0}".format(run_disk_checks)
        ansible_cmd.append(ansible_option)

    if action != "update" and (vault_reconfigure or (vault_config and vault_config['enabled'])):
        ansible_option = ansible_options + "-eRUN_CONTAINER_EXITED_CHECK=True"
        ansible_cmd.append(ansible_option)

    default_action = orchestrator.run_args.get('action', 'install')
    run_args_id = orchestrator.run_args.get('id', None)
    exec_plan_playbooks = get_all_exec_playbooks(default_action, run_args_id)

    # The basis for all execution hinges on 2 things
    # 1 - the tag whether it is base or central-ceph
    # 2 - the presence of a relevant target host coming from inventory
    if (default_action == "install" and (playbook in ansible_playbook or
            ansible_playbook in exec_plan_playbooks)) \
            and tags is None:
        if pod_type and pod_type == 'ceph':
            tag_info = ['central-ceph']
        else:
            tag_info = ['base']
            mechanism_drivers = parsed_userinput. \
                get_data_from_userinput_file(['MECHANISM_DRIVERS'])

            optional_services = parsed_userinput. \
                get_data_from_userinput_file(['OPTIONAL_SERVICE_LIST'])
            tag_info.extend(optional_services) if optional_services is not None else []
            # Add the right mechanism driver tag
            tag_info.append(mechanism_drivers)

            if parsed_userinput.create_sriov():
                tag_info.append("neutron-sriov")

            if parsed_userinput.check_section_exists('SWIFTSTACK'):
                tag_info.append("cinder-backup")

            if nfvimon_helper.is_nfvimon_enabled():
                tag_info.append('ceilometer')
                tag_info.append('nfvimon')

            if parsed_userinput.check_section_exists('NETWORK_OPTIONS'):
                net_opts = parsed_userinput.get_data_from_userinput_file(
                    ['NETWORK_OPTIONS'])
                if net_opts and ('vxlan' in net_opts or 'sr-mpls' in net_opts):
                    tag_info.append('gobgp')

        tag = ",".join(tag_info)
        orchestrator.log.debug("Add tags %s to ANSIBLE CMD", tag)
        ansible_option = ansible_options + "--tags=%s" % tag
        ansible_cmd.append(ansible_option)

    if tags is not None:
        orchestrator.log.debug("Add tags %s to ANSIBLE CMD", tags)
        ansible_option = ansible_options + "--tags=%s" % tags
        ansible_cmd.append(ansible_option)

    if skip_tags is not None:
        orchestrator.log.debug("Add skip-tags %s to ANSIBLE CMD", skip_tags)
        ansible_option = ansible_options + "--skip-tags=%s" % skip_tags
        ansible_cmd.append(ansible_option)

    if nfvimon_helper.is_nfvimon_enabled():
        nfvimon_uuid = nfvimon_helper.fetch_uuid_from_mercury_api()
        ansible_option = ansible_options + "-eNFVIMON_UUID=%s" % nfvimon_uuid
        ansible_cmd.append(ansible_option)

    orchestrator.log.debug("ANSIBLE CMD: %s", ansible_cmd)
    return ansible_cmd


def execute_ansible_playbooks(orchestrator, run_args):
    """
    Execute Ansible Playbooks.
    """
    results = {}
    results['status'] = 'PASS'

    action = run_args.get('action', None)

    homedir = get_homedir()
    cfg_dir = os.path.join(homedir, DEFAULT_CFG_DIR)

    if not os.path.exists(cfg_dir):
        print_ansible_usage()
        sys.exit(0)

    staging_secrets_file = os.path.join(cfg_dir, STAGING_SECRETS_FILE)
    default_action = run_args.get('action', 'install')
    run_args_id = run_args.get('id', None)
    exec_plan = load_policy_exec_plan(default_action, run_args_id)
    if exec_plan:
        results = perform_parallel_execution(orchestrator, run_args, exec_plan)
    else:
        results = perform_serial_execution(orchestrator)

    if action == "reconfigure" and results['status'] == 'PASS':
        if os.path.isfile(staging_secrets_file):
            os.remove(staging_secrets_file)

    return results


def perform_serial_execution(orchestrator, queue=None):
    results = {'status': 'PASS'}
    orchestrator.log.debug("Executing Ansible Playbook")

    homedir = get_homedir()
    cfg_dir = os.path.join(homedir, DEFAULT_CFG_DIR)

    if not os.path.exists(cfg_dir):
        print_ansible_usage()
        sys.exit(0)

    setup_file = os.path.join(cfg_dir, DEFAULT_SETUP_FILE)
    defaults_file = os.path.join(cfg_dir, DEFAULTS_FILE)
    secrets_file = os.path.join(cfg_dir, SECRETS_FILE)
    staging_secrets_file = os.path.join(cfg_dir, STAGING_SECRETS_FILE)
    os_config_file = os.path.join(cfg_dir, OS_CONFIG_FILE)

    if not os.path.exists(setup_file) or \
            not os.path.exists(defaults_file) or \
            not os.path.exists(os_config_file) or \
            not os.path.exists(secrets_file):
        print_ansible_usage()
        sys.exit(0)

    exec_context = orchestrator.run_args.get('exec_context', "")
    if os.path.exists(staging_secrets_file) and exec_context != "validation":
        secret_file = staging_secrets_file
    else:
        secret_file = secrets_file

    if orchestrator.run_args.get('vault_reconfigure', None):
        setup_file = os.path.join(cfg_dir, BACKUP_SETUP_FILE)

    # Docker/service manifest file.
    docker_file = os.path.join(cfg_dir, "docker.yaml")
    if not os.path.exists(docker_file):
        docker_file = os.path.join(
            os.path.dirname(common.get_absolute_path_for_file(
                __file__, "tmp", "clouddeploy")), "system_configs",
            "docker", "docker.yaml")

    orchestrator.log.debug("CONFIG Files [setup: %s], [userdata: %s], \
                           [secrets: %s ], [docker: %s] , [os_config: %s]",
                           setup_file, defaults_file,
                           secrets_file, docker_file, os_config_file)


    if not os.path.exists(docker_file):
        print "Docker file does not exist %s", docker_file
        sys.exit(0)

    playbook_dir = os.path.join(
        os.path.dirname(__file__), "playbooks")

    orchestrator.log.debug("Playbook Dir: %s", playbook_dir)

    # Get the playbook to run from the user.
    playbook_file = orchestrator.run_args.get('playbook', None)
    if playbook_file is None:
        orchestrator.log.debug("Use default playbook: openstack-install.yaml")
        playbook_file = "openstack-install.yaml"

    ansible_playbook = os.path.join(
        os.path.dirname(common.get_absolute_path_for_file(
            __file__, "tmp")), "playbooks", playbook_file)

    if not os.path.exists(ansible_playbook):
        orchestrator.log.error(logger.stringc("File %s does not exist", "red"),
                               ansible_playbook)
        results = {'status', 'FAIL'}
        if queue:
            queue_results = {orchestrator.run_args.get('log'): {'status', 'FAIL'}}
            queue.put(queue_results)
        return results

    playbook_env = os.environ.copy()
    logfile = orchestrator.run_args.get('log', None)

    default_action = orchestrator.run_args.get('action', 'install')
    default_log_path = orchestrator.run_args.get('log_path', "/var/log/mercury/")
    if queue:
        playbook_env["ANSIBLE_LOG_PATH"] = "%s/%s_os_%s.log" % (
            default_log_path, logfile, default_action)

    # Generate Ansible command.
    ansible_cmd = build_ansible_cmd(orchestrator, setup_file,
                                    defaults_file, secret_file,
                                    docker_file, os_config_file,
                                    ansible_playbook)

    # Start ansible playbook as a subprocess.
    sproc = subprocess.Popen(ansible_cmd,
                             cwd=playbook_dir,
                             stdout=subprocess.PIPE,
                             env=playbook_env)
    console_logging = orchestrator.loginst.\
        loggerconfig.get_module_console_logging(orchestrator.name)

    while True:
        try:
            nextline = sproc.stdout.readline()
            if nextline == '' and sproc.poll() is not None:
                if sproc.returncode:
                    results['status'] = 'FAIL'
                break

            results = monitor_and_validate_ansible_output(
                orchestrator, nextline, results)
            if console_logging:
                sys.stdout.write(nextline)
                sys.stdout.flush()

            if results['status'] == 'FAIL':
                sproc.kill()
                break

        except KeyboardInterrupt:
            sproc.kill()
            set_run_result(results, 'status', 'FAIL')
            set_run_result(results, 'err_msg', "Installer killed by user")

    if queue:
        queue.put({logfile: results})
    return results


def perform_parallel_execution(orchestrator, run_args, exec_plan):

    results = {'status': 'PASS'}
    for exec_group in exec_plan:
        start_time = time.time()
        results = launch_ansible_workers(run_args, exec_group)
        end_time = time.time()
        elapsed_time = int(end_time - start_time)
        mins, secs = divmod(elapsed_time, 60)
        orchestrator.log.debug("[ORCHESTRATION]: Completed group: [%i] %s in [%imin %isecs]"
                               % (int(exec_group['id']), exec_group['name'], mins, secs))
        # if any of the results returns a FAIL status,
        # return the group results right away
        if results['status'] == 'FAIL':
            return results
    return results


def launch_ansible_workers(run_args, exec_group):
    workers = []
    queue = Queue.Queue()
    for playbook in exec_group['playbooks']:
        worker_orch = Orchestrator()
        worker_orch.run_args = run_args
        worker_orch.run_args['playbook'] = playbook['name']
        worker_orch.run_args['log'] = playbook['log']
        worker = Thread(target=perform_serial_execution,
                        args=(worker_orch, queue))
        worker.start()
        workers.append(worker)

    results = []
    for worker in workers:
        worker.join()
        results.append(queue.get())

    if all([result.values()[0].get('status') == 'PASS' for result in results]):
        return {'status': 'PASS'}
    else:
        return {'status': 'FAIL'}


def run(run_args={}):
    '''
    Run method is invoked from the runner.
    '''

    orchestrator = Orchestrator()
    orchestrator.run_args = run_args

    results = {'status': 'PASS'}
    resobj = {}
    set_run_result(resobj, 'status', 'PASS')

    if os.path.isfile(INVENTORY_FILE):
        with open(INVENTORY_FILE, "r") as f:
            inv_hosts = json.load(f)    # nosec
            if len(inv_hosts['host_all']['hosts']) == 0:
                orchestrator.log.debug("Host list is empty.")
                results['status'] = 'FAIL'
                return results

    results = execute_ansible_playbooks(orchestrator, run_args)

    if results['status'] != 'PASS':
        orchestrator.log.debug("Ansible Playbook Execution failed")
        return results
    else:
        tags = orchestrator.run_args.get('tags', None)
        playbook = orchestrator.run_args.get('playbook', None)
        controller = orchestrator.run_args.get('replace_controller', None)
        opid = orchestrator.run_args.get('id', 0)
        # Tags are empty during install and reconfigure.
        configparser = config_parser.YamlHelper()
        if not configparser.get_k8scp_support():
            return results
        kube_control = kube_control_cluster_deploy.KubeDeployControl()
        if playbook and playbook == 'openstack-reconfigure.yaml':
            pass
        elif controller and opid == 7:
            print "controller :" + ' '.join(controller)
            yaml_file = common.get_cobbler_data_file_path()
            cobbler_file = config_parser.YamlHelper(user_input_file=yaml_file)
            cobbler_file_dict = cobbler_file.create_parsed_yaml(yaml_file)
            controller_ipaddr = (cobbler_file_dict[controller[0]]['bonds']
                             ['management']['ipaddress'])
            action = {}
            action['name'] = 'replace_controller'
            action['details'] = controller_ipaddr
            if kube_control.deploy_cluster(action):
                orchestrator.log.debug("Kubespray Ansible Playbook Execution failed")
                results['status'] = 'FAIL'
        elif opid == 7:
            if kube_control.deploy_cluster():
                orchestrator.log.debug("Kubespray Ansible Playbook Execution failed")
                results['status'] = 'FAIL'

    return results

def check_status():
    '''
    checkstart.
    '''
    return (OrchestratorStatus.STAGE_COUNT,
            OrchestratorStatus.OPER_STAGE)


def main():
    '''
    main. Only to be invoked for manual test runs.
    '''
    print "In main"
    run()

if __name__ == '__main__':
    main()
