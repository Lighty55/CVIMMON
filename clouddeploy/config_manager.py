#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Config Manager:
==============

What is the Role of the config Manager?
---------------------------------------
Cisco VIM deployer provides a Bare-metal openstack installation/deployment.
The modules are modular enough to run in itself:

High Level Modules:
-------------------
1. Baremetal installer.
    - Performs Server Discovery, VNIC Creation, RAID Management,
    OS Install using cobbler.
2. Config Manager.
   - Config Manager is the piece that holds the configurations in place
     - There are two main types of configuration input:
         * User Input:
         =============
             - Server Setup info:
               -----------------
                 Details server CIMC info for all servers in the racks.
                 - Cobbler info
                 - Networking details:
                     - Global info like DNS, proxies, ntp server info etc.
                     - specifies the various vnics for traffic segmentation with
                       vlan information, ip and subnet info.
                 - Specifications of Roles for servers.
             - Openstack configuration info:
                ---------------------------
                 - We will expose the key pieces of configuration that will be
                 used during openstack installation. Like username and passwords
                 individual services like rabbitmq, MariaDB, etc.

         * System configuration
         =======================
        System configuration is meant not to be modified by the user.
            - Roles and Profiles:
            ---------------------
            Roles and Profiles define the various roles for the servers. Each
            role specifies what packages will be installed, what services will
            be running on the specific hosts.

            - Service Configurations:
            -------------------------
            Cisco VIM installer uses Docker containerized services where each
            openstack services is running in it's own docker container.
            The service configuration is where the details of each service is
            specified.
            A service configuration for a service will specify:
                - Which Docker Image to Use.
                - A docker register/private Repo to get the image from.
                - image_id
                - Staging directory on the host where the service will run.
                - Service count - specifies the instances of the service
                spreaad across the pool of designated hosts (eg: controllers)

            - Openstack Manifest:
            ---------------------
            Openstack Manifest is a (GENERATED) file. It is generated by
            config_manager. It can be invoked manually or it can be called
            from the baremetal install as the last step after server discovery.

            The openstack manifest is a consolidated view of the entire system
            which it captures from all the various configurations.

            - Ansible Inventory:
            ---------------------
            We will use ansible dynamic inventory during the installation
            phase. Ansible dynamic inventory  script will generate the inventory
            object from the openstack manifest file.

3. Orchestrator:
    An orchestrator allows to orchestrate the varous services running in
    docker containers on the pool of designated servers.
    The basic functionality it will have to begin with is "INSTALL". In this
    mode it will invoke the installer to install all the openstack services.
    It can be run with other Modes:
        "upgrade" which can be used to upgrade specific services.
        "manage" which will monitor the services and make sure that at any time
        the specified services are running in correct mode as specified in the
        configurations. It can detect failed services and orchestrate starting
        service on other nodes. This will also require that the other services
        be updated with the new info.

4. Installer:
    Does what it says, it installs openstack on the user setup. It uses Ansible
    framework and goes through all the openstack playbooks to install and
    configure all the services on the nodes.

    It will be modular enough to install only a specific service.
    Eg. It can be called to install only RabbitMQ, or Galera .
    If a service  requires other services, then it will install them as well.
    eg: Keystone service will install Rabbit, Galera first
"""
import os
import json
import itertools
import StringIO
import ConfigParser
import uuid
import netifaces
import hashlib
import re
import six
import socket
import utils.common as common
import utils.logger as logger
import utils.config_parser as config_parser

from openstackclients import osnetwork_client as nclient

import netaddr
from utils.vtc_cfg.vtc_client import VtcClient

CEPH_CLUSTER_ID_CONF = os.path.join(os.environ['HOME'],
                                "openstack-configs/ceph/fetch/ceph_cluster_uuid.conf")
INSTALLER_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

DOCKER_MAX_SERVICE_COUNT = 500


class ConfigManager(object):
    '''
    Configuration Management.
    '''
    def __init__(self, userinput=None, via_softlink=1):
        '''
        Initialize config manager.
        '''
        ################################################
        # Set up logging
        ###############################################
        self.loginst = logger.Logger(name=__name__)
        self.log = self.loginst.get_logger()

        self.parsed_user = None
        self.bonds = {}

        if userinput is None:
            self.log.error("No input file specified. Requires a config file")
            return
        self.userfile = common.get_absolute_path_for_file(__file__,
                                                          userinput,
                                                          splitdir="clouddeploy")

        if via_softlink:
            self.defaults_file = common.get_defaults_file_path()
        else:

            file_name_list = common.find_file_path(INSTALLER_ROOT, "defaults.yaml")
            default_file_abs_path = ""

            for item in file_name_list:
                if os.path.basename(item) == 'defaults.yaml':
                    default_file_abs_path = item
                    break

            defaults_file = default_file_abs_path
            self.defaults_file = common.get_defaults_file_path(defaults_file)

        self.log.debug("User Input: %s", self.userfile)

        self.ymlhelper = config_parser.YamlHelper(user_input_file=self.userfile)
        self.parsed_defaults = \
            config_parser.YamlHelper(user_input_file=self.defaults_file)

        if self.ymlhelper is None or self.ymlhelper.parsed_config is None:
            self.log.error("Could not Parse configs")
            return

    def _locate_install_dir(self):
        # Step up each directory name until 'clouddeploy/playbooks'
        # is found in the subdir path or nothing left to the path.
        # This will identify the install path.
        install_path = os.getcwd()
        while (os.path.exists(install_path + '/clouddeploy/playbooks') is False and
               install_path != '/'):
            install_path = os.path.dirname(install_path)
        return install_path

    def get_timezone(self):
        '''
        Derive timezone from management node. If it cannot
        be derived, default to UTC
        '''
        try:
            return os.readlink('/etc/localtime').split('/usr/share/zoneinfo/')[1]
        except IOError as ierr:
            self.log.error("Error: Timezone cannot be derived. Using UTC timezone")
            return "UTC"

    def use_legacy_storage_net(self):
        '''Uses legacy storage network'''
        backward = False
        try:
            cobbler_file = config_parser.YamlHelper(
                user_input_file=common.get_cobbler_data_file_path())
            servers = cobbler_file.parsed_config.keys()
            for server in servers:
                server_info = cobbler_file.parsed_config[server]
                if ("vts" not in server_info['role'].split()) and \
                        ("storage" not in server_info.get('bonds') or \
                            "use_legacy" in server_info):
                    backward = True
                    break
        except IOError:
            return False
        return backward

    def generate_openstack_manifest(self, gen_file=None):
        '''
        Generate the openstack Manifest file.
        '''
        if gen_file is None:
            gen_file = common.get_absolute_path_for_file(
                __file__,
                "../system_configs/_openstack_manifest.yaml")

        # If file exist, should we create a copy.
        self.log.debug("Generate openstack Manifest. Start")
        if not self.should_generate_inventory():
            if os.path.isfile(gen_file):
                with open(gen_file) as data_file:
                    manifest_data = json.load(data_file)
                    return manifest_data

        manifest_data = {}
        manifest_data['_meta'] = {}
        manifest_data['_meta']['hostvars'] = {}
        hostvars = manifest_data['_meta']['hostvars']

        serverlist = self.ymlhelper.get_server_list()
        for server in serverlist:
            if self.get_server_bonds(server):
                server_info = self.build_server_info_dict(server, allroles=True)
                hostvars[server] = server_info

        # Definition of services and what roles they belong.
        control_servicelist = ['rabbitmq', 'mariadb', 'haproxy',
                               'keystone', 'glance_registry', 'glance_api',
                               'nova_conductor', 'nova_scheduler', 'nova_api',
                               'nova_consoleauth', 'nova_novncproxy', 'ironic',
                               'neutron_dhcp_agent', 'neutron_l3_agent',
                               'neutron_lbaas_agent', 'neutron_server',
                               'neutron_metadata_agent', 'horizon', 'opsmonit',
                               'cinder_api', 'cinder_scheduler',
                               'heat_api', 'heat_api_cfn', 'heat_engine',
                               'cinder_volume', 'cloudpulse_server',
                               'ceilometer_central',
                               'ceilometer_notification',
                               'magnum_api', 'magnum_conductor',
                               'etcd', 'gobgp', 'cvim_proxy', 'calipso',
                               'gnocchi_api', 'gnocchi_metricd', 'gnocchi_statsd']

        # Compute services (only on compute nodes)
        compute_servicelist = ['nova_libvirt', 'nova_compute', 'nova_ssh',
                               'ceilometer_compute', 'neutron_sriov_agent']

        # Storage nodes. This group will be needed if LVM is run
        # in a production environment. We should consider distinguishing
        # between openstack volume role and ceph storage role in the
        # future we can make controller nodes also have the openstack
        # volume role while ceph role is managed separately.
        block_storage_servicelist = ['ceph_osd', 'fluentd']

        # Common services (control and compute - But not storage)
        common_servicelist = ['neutron_common', 'nova_common',
                              'neutron_linuxbridge_agent', 'ovs', 'fluentd',
                              'neutron_vpp', 'neutron_vtf']

        # For CEPH POD Type
        cephosd_servicelist = ["ceph_osd", "fluentd"]
        cephcontrol_servicelist = ["ceph_mon", "ceph_mgr", "fluentd", "cvim_proxy"]

        # Excusive list
        # The services in this list cannot be placed on the same host.
        # For services in the set the second service in the set
        # will be placed on hosts not part of the first service.
        # Which would mean that it will override the service count.
        exclusivity_list = []
        exclusivity_set = ('haproxy', 'neutron_l3_agent')
        exclusivity_list.append(exclusivity_set)

        if self.ymlhelper.get_pod_type() != 'ceph':
            for service in control_servicelist:
                service_key = service + "_all"
                manifest_data[service_key] = self.build_service_info_dict(
                    service, role='control')
                service_key = service + "_mgmt_ip"
                manifest_data[service_key] = self.build_service_info_dict(
                    service, role="control", bond="control")
        else:
            # For ceph pod type only, populate service information into
            # manifest based on the cephosd and cephcontrol service groups.
            # For any other type of pod, do not overwrite the existing
            # manifest data.
            for service in cephosd_servicelist:
                service_key = service + "_all"
                manifest_data[service_key] = self.build_service_info_dict(
                    service, role='cephosd')
                service_key = service + "_mgmt_ip"
                manifest_data[service_key] = self.build_service_info_dict(
                    service, role="cephosd", bond="cephosd")

            for service in cephcontrol_servicelist:
                service_key = service + "_all"
                manifest_data[service_key] = self.build_service_info_dict(
                    service, role='cephcontrol')
                service_key = service + "_mgmt_ip"
                manifest_data[service_key] = self.build_service_info_dict(
                    service, role="cephcontrol", bond="cephcontrol")

        for service in compute_servicelist:
            service_key = service + "_all"
            current_service_info_dict = manifest_data.get(service_key, {})
            new_service_info_dict = self.build_service_info_dict(
                service, role='compute')
            manifest_data[service_key] = self.add_service_info_dict(\
                current_service_info_dict, new_service_info_dict)
            service_key = service + "_mgmt_ip"
            manifest_data[service_key] = self.build_service_info_dict(
                service, role="compute", bond="control")

        for service in compute_servicelist:
            service_key = service + "_power_all"
            current_service_info_dict = manifest_data.get(service_key, {})
            new_service_info_dict = self.build_service_info_dict(
                service, role='compute', bond=None, with_power_status=1)
            manifest_data[service_key] = self.add_service_info_dict(\
                current_service_info_dict, new_service_info_dict)
            service_key = service + "_mgmt_ip"
            manifest_data[service_key] = self.build_service_info_dict(
                service, role="compute", bond="control")

        manifest_data["controllers"] = self.ymlhelper.get_server_list(role="control")
        manifest_data["compute"] = self.ymlhelper.get_server_list(role="compute")
        manifest_data["mgmt"] = []
        manifest_data["mgmt"].append(socket.gethostname())

        if self.ymlhelper.get_server_list(role="cephosd"):
            manifest_data["cephosd"] = self.ymlhelper.get_server_list(\
                role="cephosd")
        if self.ymlhelper.get_server_list(role="cephcontrol"):
            manifest_data["cephcontrol"] = self.ymlhelper.get_server_list(\
                role="cephcontrol")

        if self.ymlhelper.get_server_list(role="block_storage"):
            manifest_data["storage"] = self.ymlhelper.get_server_list(\
                role="block_storage")
            if self.use_legacy_storage_net():
                manifest_data["mon_hosts"] = self.build_service_info_dict(
                    service, role="control", bond="management")
            else:
                manifest_data["mon_hosts"] = self.build_service_info_dict(
                    service, role="control", bond="storage")
        # This logic should be enabled if LVM is run in production, as
        # cinder volume should run in separate storage nodes with
        # high density storage.
        for service in block_storage_servicelist:
            service_key = service + "_all"
            current_service_info_dict = manifest_data.get(service_key, {})
            new_service_info_dict = self.build_service_info_dict(
                service, role='block_storage')
            manifest_data[service_key] = self.add_service_info_dict(current_service_info_dict,
                                                                    new_service_info_dict)
        #    service_key = service + "_mgmt_ip"
        #    manifest_data[service_key] = self.build_service_info_dict(
        #        service, role="block_storage", bond="control")

        for service in common_servicelist:
            service_key = service + "_power_all"
            current_service_info_dict = manifest_data.get(service_key, {})
            new_service_info_dict = self.build_service_info_dict(
                service, role='common', bond=None, with_power_status=1)
            manifest_data[service_key] = self.add_service_info_dict(current_service_info_dict,
                                                                    new_service_info_dict)
            service_key = service + "_all"
            new_service_info_dict = self.build_service_info_dict(
                service, role="common")
            current_service_info_dict = manifest_data.get(service_key, {})
            manifest_data[service_key] = self.add_service_info_dict(current_service_info_dict,
                                                                    new_service_info_dict)
            service_key = service + "_mgmt_ip"
            manifest_data[service_key] = self.build_service_info_dict(
                service, role="common", bond="control")

        # Additional Groups.
        # host_all group var.
        manifest_data['host_all'] = self.build_service_info_dict(
            'host_all', role=None)
        manifest_data['host_power_all'] = self.build_service_info_dict(
            'host_all', role=None, with_power_status=1)
        manifest_data['host_all_mgmt_ip'] = self.build_service_info_dict(
            'host_all', role=None, bond="control")
        #manifest_data['host_control_api_ips'] = self.build_service_info_dict(
        #    'host_all', role="control", bond="api")
        manifest_data['host_compute_tenant_ips'] = \
            self.build_service_info_dict(\
                'host_all', role="compute", bond="tenant")
        manifest_data['host_power_all_compute_tenant_ips'] = \
            self.build_service_info_dict(\
            'host_all', role="compute", bond="tenant", with_power_status=1)

        # host_common group var
        # All control and computes.
        manifest_data['host_common'] = self.build_service_info_dict(\
            'host_common', role="common")
        manifest_data['host_control'] = self.build_service_info_dict(\
            'host_control', role="control")
        manifest_data['host_control_mgmt_ip'] = self.build_service_info_dict(\
            'host_control', role="control", bond="control")

        # All NFV hosts
        manifest_data['nfv_hosts'] = self.build_service_info_dict('nfv_hosts',
                                                                  role='nfv_host')

        # Storage interface IP for all powered on hosts
        storage_ip_list = self.get_storage_ip_list()
        if storage_ip_list:
            manifest_data['storage_ip_power_all'] = storage_ip_list


        # Cluster/replication interface IP for all ceph pod osd nodes
        cluster_ip_list = self.get_cluster_ip_list()
        if cluster_ip_list:
            manifest_data['cluster_ip_all'] = cluster_ip_list

        # Get APIC controller IPs if mechanism driver is ACI
        if self.ymlhelper.get_mechanism_driver() == "aci":
            apic_hosts = self.ymlhelper.get_apic_info()['apic_hosts']
            if apic_hosts:
                manifest_data['apic_hosts'] = apic_hosts

        # Post processing on service groups.
        # Go through exclusivity list.
        control_list = self.ymlhelper.get_server_list(role="control")
        # will retain the code for placement logic
        if False:
            if len(control_list) == 3:
                self.log.debug("Perform manifest post processing")
                for exc_set in exclusivity_list:
                    # Fix the ssh hosts ip list
                    skey1 = exc_set[0] + "_all"
                    skey2 = exc_set[1] + "_all"
                    hostsdiff = set(manifest_data['host_control']['hosts']) - \
                        set(manifest_data[skey1]['hosts'])
                    print "intersect: ", hostsdiff
                    manifest_data[skey2]['hosts'] = list(hostsdiff)

                    # Fix the  mgmt hosts ip list.
                    skey1 = exc_set[0] + "_mgmt_ip"
                    skey2 = exc_set[1] + "_mgmt_ip"
                    hostsdiff = set(manifest_data['host_control_mgmt_ip']['hosts']) - \
                        set(manifest_data[skey1]['hosts'])
                    manifest_data[skey2]['hosts'] = list(hostsdiff)
            else:
                self.log.debug("control nodes %d != 3. Skip post processing",
                               len(control_list))
        else:
            self.log.debug("Post processing disabled.")

        # Management server info.
        server_list = self.ymlhelper.get_server_list()
        cobbler_file = config_parser.YamlHelper(
            user_input_file=common.get_cobbler_data_file_path())
        create_sriov = self.ymlhelper.create_sriov()
        use_same_physnet = \
            self.parsed_defaults.parsed_config.get('USE_SAME_PHYSNET')
        physnet = \
            self.parsed_defaults.parsed_config.get('SRIOV_PHYSNET_NAME')
        interface = \
            self.parsed_defaults.parsed_config.get('SRIOV_INTERFACE_NAME')
        fpga_physnet = \
            self.parsed_defaults.parsed_config.get('FPGA_PHYSNET_NAME')
        fpga_interface = \
            self.parsed_defaults.parsed_config.get('FPGA_INTERFACE_NAME')

        pure_intel_pids = []
        for server in server_list:
            svr_dict = self.ymlhelper.get_server_info(server)
            # check whether the user has provided bond info
            bonds = self.get_server_bonds(server)
            if not bonds:
                continue
            server_info = {}

            # Add role to servers.
            chostlist = self.ymlhelper.get_server_list(role="control")
            complist = self.ymlhelper.get_server_list(role="compute")
            strglist = self.ymlhelper.get_server_list(role="block_storage")
            vtslist = self.ymlhelper.get_server_list(role="vts")
            cephosdlist = self.ymlhelper.get_server_list(role="cephosd")
            cephctrllist = self.ymlhelper.get_server_list(role="cephcontrol")
            server_info['server_role'] = []
            if server in chostlist:
                server_info['server_role'].append('control')
            if server in complist:
                server_info['server_role'].append('compute')
            if server in vtslist:
                server_info['server_role'].append('vts')
            if server in strglist:
                server_info['server_role'].append('block_storage')
            if server in cephosdlist:
                server_info['server_role'].append('cephosd')
            if server in cephctrllist:
                server_info['server_role'].append('cephcontrol')

            # Setup bonds information for each server.
            manifest_data[bonds['management']['ipaddress']] = {}
            # control bond.
            server_info['control_bond'] = bonds['management']['ipaddress']
            if bonds['management'].get('ipv6_address'):
                server_info['management_ipv6'] = bonds['management'][
                    'ipv6_address']
                server_info['ansible_host'] = bonds['management'][
                    'ipv6_address']
            else:
                server_info['ansible_host'] = bonds['management']['ipaddress']

            # api bond.
            server_info['hostname'] = server
            if self.ymlhelper.get_pod_type() != "ceph":
                server_info['api_gw'] = \
                    self.ymlhelper.nw_get_specific_vnic_info('api', 'gateway')
                subnet = self.ymlhelper.nw_get_specific_vnic_info('api', 'subnet')
                if '/' in subnet:
                    mask = subnet.split('/')
                    server_info['api_cidr'] = mask[1]

                # IPv6 api bond
                api_ipv6_subnet = self.ymlhelper.nw_get_specific_vnic_info(
                    'api', 'ipv6_subnet')
                api_ipv6_gw = self.ymlhelper.nw_get_specific_vnic_info(
                    'api', 'ipv6_gateway')
                if api_ipv6_subnet and api_ipv6_gw:
                    server_info['api_ipv6_subnet'] = api_ipv6_subnet
                    server_info['api_ipv6_gw'] = api_ipv6_gw
                    if '/' in api_ipv6_subnet:
                        server_info['api_ipv6_subnet_len'] = \
                            api_ipv6_subnet.split('/')[-1]

            # IPaddress on storage interface
            if 'storage' in cobbler_file.parsed_config[server]['bonds']:
                server_info['storage_ip'] = \
                    cobbler_file.parsed_config[server]['bonds']['storage']['ipaddress']

            # IPaddress on cluster/replication interface for ceph pod
            if 'cluster' in cobbler_file.parsed_config[server]['bonds']:
                server_info['cluster_ip'] = \
                    cobbler_file.parsed_config[server]['bonds']['cluster']['ipaddress']

            # user has defined a def route
            def_route = self.ymlhelper.get_default_route()
            if def_route:
                server_info['def_route'] = def_route
            else:
                server_info['def_route'] = \
                    self.ymlhelper.nw_get_specific_vnic_info('management',
                                                             'gateway')
            # L3 HA
            if len(self.ymlhelper.get_server_list(role="control")) > 1:
                server_info['l3_ha'] = "True"
            else:
                server_info['l3_ha'] = "False"

            # Check if GPU node or not
            server_info['gpu_count'] = self.ymlhelper.get_gpu_count(server)
            server_info['vgpu_type'] = self.ymlhelper.get_vgpu_type(server)

            nic_sriov = \
                cobbler_file.parsed_config[server].get('create_sriov')
            if nic_sriov:
                server_info['nic_sriov'] = True
                server_info['intel_sriov_vfs'] = \
                    self.ymlhelper.get_intel_sriov_vfs(server)
                server_info['intel_vc_sriov_vfs'] = \
                    self.ymlhelper.get_intel_vc_sriov_vfs(server)
            else:
                server_info['nic_sriov'] = False

            server_info['intel_fpga_vfs'] = \
                self.ymlhelper.get_intel_fpga_vfs(server)

            server_info['use_intel'] = False
            mech_driver = self.ymlhelper.get_mechanism_driver()
            server_info['vic_nic_combo'] = False

            # Intel SRIOV mac-addresses
            if create_sriov:
                bm_inv_file = common.get_baremetal_inventory()
                if os.path.isfile(bm_inv_file):
                    bm_yaml = config_parser.YamlHelper(user_input_file=bm_inv_file)
                    if bm_yaml:
                        mac_data = bm_yaml.create_parsed_yaml(bm_inv_file)
                        if server in mac_data.keys():
                            sriov_mac_addrs = mac_data[server]['macs']['sriov']
                            if len(sriov_mac_addrs) > 0:
                                server_info['sriov_macs'] = sriov_mac_addrs
                            if (mac_data[server]['macs'].get('fpga') and
                                    len(mac_data[server]['macs']['fpga']) > 0):
                                server_info['fpga_macs'] = mac_data[server][
                                    'macs']['fpga']

            # Determine card_type
            vid, pid = self.ymlhelper.get_intel_vendor_and_product_id(server)
            if vid and pid:
                server_info['VENDOR_ID'] = vid
                server_info['PRODUCT_ID'] = pid
                pure_intel_pids.append(pid)

            # NOTE: The following order of evaluation is important and
            # should NOT be altered i.e. is_cisco_vic_intel_sriov()
            # must always be evaluated before the use_intel_nic()
            if self.ymlhelper.is_cisco_vic_intel_sriov():
                server_info['vic_nic_combo'] = True
                if "provider" in self.ymlhelper.nw_get_vnic_segments():
                    server_info['provider_exists'] = True
                    # NOTE: Special case for HP compute since its behaving
                    #       like VIC-NIC combo setup with only NICs.
                    if (self.ymlhelper.use_intel_nic(server) and
                            config_parser.PlatformDiscovery().is_thirdparties(
                                server)):
                        server_info["prov_if"] = "prov"
                else:
                    server_info['provider_exists'] = False
                # NOTE: Special case for HP controller since its behaving like
                #       VIC-NIC combo setup with only NICs.
                if (self.ymlhelper.use_intel_nic(server) and
                        "control" in server_info["server_role"] and
                        config_parser.PlatformDiscovery().is_thirdparties(
                            server)):
                    cp_intf = self.parsed_defaults.parsed_config[
                        "CONTROL_INTERFACE_NAME"]
                    dp_intf = self.parsed_defaults.parsed_config[
                        "DATA_INTERFACE_NAME"]
                    ex_vlan = self.ymlhelper.nw_get_specific_vnic_info(
                        "external", "vlan_id")
                    server_info["external_if"] = "%s%s.%s" % (cp_intf, dp_intf,
                                                              ex_vlan)
            elif self.ymlhelper.use_intel_nic(server):
                server_info['use_intel'] = True
                server_info["tenant_if"] = \
                    self.parsed_defaults.parsed_config['DATA_INTERFACE_NAME']
                server_info["control_if"] = \
                    self.parsed_defaults.parsed_config[
                        'CONTROL_INTERFACE_NAME']

                if mech_driver.lower() == "openvswitch":
                    ex_vlan = self.ymlhelper.nw_get_specific_vnic_info(
                        "external", "vlan_id")
                    server_info["prov_if"] = "p"
                    server_info['provider_exists'] = False
                    server_info["external_if"] = \
                        server_info["control_if"] + "." + str(ex_vlan)
                elif mech_driver.lower() == "linuxbridge":
                    if "provider" in self.ymlhelper.nw_get_vnic_segments():
                        server_info['provider_exists'] = True
                        server_info['intel_prov_interface'] = \
                            server_info["tenant_if"]
                    ext_vlan = self.ymlhelper.nw_get_specific_vnic_info(\
                        "external", "vlan_id")
                    server_info['intel_ext_interface'] = \
                        server_info["tenant_if"] + "." + str(ext_vlan)
                else:
                    server_info['provider_exists'] = False
            else:
                if "provider" in self.ymlhelper.nw_get_vnic_segments():
                    server_info['provider_exists'] = True
                else:
                    server_info['provider_exists'] = False

            # Generate tenant interface name
            if server_info['use_intel']:
                server_info['tenant_if_name'] = 'pet'
                server_info['tenant_if_slave0'] = 'pet0'
                server_info['tenant_if_slave1'] = 'pet1'
            # NOTE: Combined control+data interface scenario where tenant
            #       interface is a veth plugged into the br_mgmt bridge and
            #       the physical interface is samxpet.
            elif (self.ymlhelper.use_intel_nic(server) and \
                    server_info['vic_nic_combo']):
                server_info["tenant_if"] = self.parsed_defaults.parsed_config[
                    'DATA_INTERFACE_NAME']
                server_info['tenant_if_name'] = server_info["tenant_if"]
            else:
                server_info['tenant_if_name'] = 't'
                # Note: need to set the following as well since
                # tenant interface names are inconsistently
                # referenced in templates
                server_info['tenant_if'] = 't'
                server_info['tenant_if_slave0'] = 't0'
                server_info['tenant_if_slave1'] = 't1'

            # SRIOV Multivlan trunk
            sriov_multivlan_list = self.ymlhelper.get_sriov_multivlan_trunk_info()
            if sriov_multivlan_list is not None:
                server_info['trunk_mvlan_w_sriov'] = sriov_multivlan_list

            # API
            if bonds.get('api', None):
                if bonds.get('api').get('mask', None):
                    server_info['api_cidr'] = self.netmask_to_cidr(
                        bonds['api']['mask'])
                else:
                    subnet = self.ymlhelper.nw_get_specific_vnic_info(
                        'api', 'subnet')
                    if '/' in subnet:
                        mask = subnet.split('/')
                        server_info['api_cidr'] = mask[1]
                # IPv6
                api_ipv6_subnet = self.ymlhelper.nw_get_specific_vnic_info(
                    'api', 'ipv6_subnet')
                if api_ipv6_subnet and '/' in api_ipv6_subnet:
                    server_info['api_ipv6_subnet_len'] = api_ipv6_subnet.split(
                        '/')[-1]

            # management
            if bonds.get('management', None):
                server_info['mgmt_gw'] = \
                    self.ymlhelper.nw_get_specific_vnic_info('management', 'gateway')
                if bonds.get('management').get('mask', None):
                    server_info['control_cidr'] = self.netmask_to_cidr(
                        bonds['management']['mask'])
                    subnet = \
                        self.ymlhelper.nw_get_specific_vnic_info('management',
                                                                 'subnet')
                else:
                    subnet = self.ymlhelper.nw_get_specific_vnic_info(
                        'management', 'subnet')

                if '/' in subnet:
                    mask = subnet.split('/')
                    server_info['control_nw'] = mask[0]
                    server_info['control_cidr'] = mask[1]
                # IPv6
                mgmt_ipv6_subnet = self.ymlhelper.nw_get_specific_vnic_info(
                    'management', 'ipv6_subnet')
                if mgmt_ipv6_subnet and '/' in mgmt_ipv6_subnet:
                    server_info['mgmt_ipv6_subnet_len'] = \
                        mgmt_ipv6_subnet.split('/')[-1]
                mgmt_ipv6_gw = \
                    self.ymlhelper.nw_get_specific_vnic_info('management', 'ipv6_gateway')
                if mgmt_ipv6_gw:
                    server_info['mgmt_ipv6_gw'] = mgmt_ipv6_gw

            # Tenant bond
            if bonds.get('tenant', None) is not None:
                server_info['vts_tenant_network'] = self.ymlhelper.nw_get_specific_vnic_info(
                    'tenant', 'subnet')
                server_info['tenant_bond'] = bonds['tenant']['ipaddress']
                server_info['tenant_gw'] = bonds['tenant'].get(
                    'gateway',
                    self.ymlhelper.nw_get_specific_vnic_info(
                        'tenant', 'gateway'))
                server_info['vts_tenant_vlan'] = self.ymlhelper.nw_get_specific_vnic_info(\
                    "tenant", "vlan_id")
                if bonds['tenant'].get('mask', None):
                    server_info['tenant_cidr'] = self.netmask_to_cidr(
                        bonds['tenant']['mask'])
                else:
                    subnet = self.ymlhelper.nw_get_specific_vnic_info(
                        'tenant', 'subnet')
                    if '/' in subnet:
                        mask = subnet.split('/')
                        server_info['tenant_cidr'] = mask[1]

            else:
                server_info['tenant_bond'] = None
                server_info['tenant_cidr'] = None

            adv_nw_cfg = common.get_adv_nw_config_file_path()
            if os.path.isfile(adv_nw_cfg):
                parser = config_parser.YamlHelper(user_input_file=adv_nw_cfg)
                if parser:
                    adv_cfg = parser.parsed_config.get('adv_nw_config', None)

            # vxlan bond
            vxlan_bonds = {x: bonds[x] for x in bonds.keys() if x[:5] == 'vxlan'}
            if vxlan_bonds:
                server_info['vxlan'] = {}
            for k, v in vxlan_bonds.items():
                vxlan_dict = {'vxlan_bgp_speaker_ip': None,
                              'vxlan_vtep_cidr': None}

                if 'compute' in server_info['server_role'] or \
                        'control' in server_info['server_role']:
                    vtep_ip = bonds[k]['compute'].get('ipaddress', None)
                    vtep_netmask = bonds[k]['compute'].get('mask', None)
                    if vtep_ip and vtep_netmask:
                        vxlan_dict['vxlan_vtep_cidr'] = '/'.join([
                            vtep_ip, str(self.netmask_to_cidr(vtep_netmask))])
                if 'control' in server_info['server_role']:
                    vxlan_dict['vxlan_bgp_speaker_ip'] = \
                        bonds[k]['control'].get('ipaddress', None)
                server_info['vxlan'].update({k: vxlan_dict})

            srmpls_bonds = {x: bonds[x] for x in bonds.keys() if x == 'sr-mpls-tenant'}
            if srmpls_bonds:
                server_info['srmpls'] = {}
                if adv_cfg is not None:
                    if adv_cfg.get(server_info['hostname'])['sr'] is not None:
                        server_info['srmpls'] = adv_cfg.get(server_info['hostname'])

            for k, v in srmpls_bonds.items():
                srmpls_dict = {'srmpls_bgp_speaker_ip': None,
                               'srmpls_tep_cidr': None}

                if 'compute' in server_info['server_role'] or \
                        'control' in server_info['server_role']:
                    vtep_ip = bonds[k]['compute'].get('ipaddress', None)
                    vtep_netmask = bonds[k]['compute'].get('mask', None)
                    if vtep_ip and vtep_netmask:
                        srmpls_dict['srmpls_tep_cidr'] = '/'.join([
                            vtep_ip, str(self.netmask_to_cidr(vtep_netmask))])
                if 'control' in server_info['server_role']:
                    srmpls_dict['srmpls_bgp_speaker_ip'] = \
                        bonds[k]['control'].get('ipaddress', None)
                server_info['srmpls'].update({k: srmpls_dict})

            # Push the needed NFV configs for Ansible to consume, set to empty
            # strings for non NFV host.
            server_info['is_nfv_host'] = self.ymlhelper.is_nfv_host(server)
            ndict = cobbler_file.parsed_config[server].get('nfv_dict', None)
            ndict = ndict['ansible_inventory_dict'] if ndict else {}
            for key in ['vmcpus', 'hostcpus', 'vswitchcpus', 'non_hostcpus',
                        'res_sys_mem', 'res_hp_2m', 'res_hp_1g',
                        'ceph_osd_cpuset_cpus', 'ceph_osd_memory_limit']:
                if key in ndict:
                    server_info[key] = ndict[key]
            # VM_HUGEPAGE_SIZE is defined in setup_data.yaml as optional, and
            # defaults to 2M in defaults.yaml. Due to Ansible is not honoring
            # the order when parsing extra variables with "-e", we are parsing
            # here as a global alias "vm_hugepage_size" to make sure
            # setup_data.yaml always override default.yaml.
            server_info['vm_hugepage_size'] = \
                self.ymlhelper.get_nfv_configs(server)['vm_hugepage_size']

            # This is to workaround the failure in the case of upgrade. When
            # upgrade is perfromed at the very beginning, Ansible is still
            # generating the inventory based on the old .cobbler_data.yaml. So
            # we are defaulting server['hostcpus'] to some values, and
            # the inventory will be updated once again when the upgrade
            # platbook is executed.
            server_info.setdefault('hostcpus', "VALUE TO BE UPDATED")

            server_info['libvirtcpus'] = server_info['hostcpus']
            if server_info.get('vmcpus'):
                server_info['libvirtcpus'] += ',' + server_info['vmcpus']

            server_info['vswitch_container_cpus'] = server_info['hostcpus']
            if server_info.get('vswitchcpus'):
                server_info['vswitch_container_cpus'] += ',' + server_info['vswitchcpus']

            server_info['nova_cpu_allocation_ratio'] = \
                self.ymlhelper.get_nova_cpu_alloc_ratio(server)
            server_info['nova_ram_allocation_ratio'] = \
                self.ymlhelper.get_nova_ram_alloc_ratio(server)

            # Add management node ip to servers.
            server_info['build_node_ip'] = self.get_build_node_ip('management')

            # TODO: Ideally, this mapping should be done at compute node during
            #       sriov agent startup instead of globally/statically defined
            #       here.  For now, without risking regression bugs since CVIM
            #       supports so many different baremetal combination, keeping
            #       it as it is but consider cleaning this up later.
            sriov_mappings = ""
            fpga_mappings = ""
            if create_sriov:
                server_info['intel_sriov'] = True
                server_info['intel_sriov_vfs'] = \
                    self.ymlhelper.get_intel_sriov_vfs(server)
                server_info['intel_vc_sriov_vfs'] = \
                    self.ymlhelper.get_intel_vc_sriov_vfs(server)
                if server_info.get('sriov_macs'):
                    server_info['sriov_phys_ports'] = len(server_info['sriov_macs'])
                    if physnet and interface:
                        sriov_ports = len(server_info['sriov_macs'])
                        if use_same_physnet:
                            for port in range(sriov_ports - 1):
                                sriov_mappings += physnet + ":" + interface + str(port) + ","
                            sriov_mappings += physnet + ":" + interface + str(sriov_ports - 1)
                        else:
                            for port in range(sriov_ports - 1):
                                sriov_mappings += physnet + str(port) + ":" + interface + str(port) + ","
                            sriov_mappings += physnet + str(sriov_ports - 1) + \
                                ":" + interface + str(sriov_ports - 1)
                else:
                    server_info['sriov_phys_ports'] = 0
                server_info['sriov_physnet_mappings'] = sriov_mappings
                # fpga nic
                if server_info.get('fpga_macs'):
                    server_info['fpga_phys_ports'] = len(server_info[
                        'fpga_macs'])
                    if fpga_physnet and fpga_interface:
                        fpga_ports = len(server_info['fpga_macs'])
                        for port in range(fpga_ports - 1):
                            fpga_mappings += fpga_physnet + str(port) + ":" + fpga_interface + str(port) + ","
                        fpga_mappings += fpga_physnet + str(fpga_ports - 1) + ":" + fpga_interface + str(fpga_ports - 1)
                else:
                    server_info['fpga_phys_ports'] = 0
                server_info['fpga_physnet_mappings'] = fpga_mappings
            else:
                server_info['intel_sriov'] = False
                server_info['sriov_physnet_mappings'] = sriov_mappings
                server_info['sriov_phys_ports'] = 0
                server_info['fpga_physnet_mappings'] = fpga_mappings
                server_info['fpga_phys_ports'] = 0

            server_info['intel_fpga_vfs'] = \
                self.ymlhelper.get_intel_fpga_vfs(server)

            manifest_data[bonds['management']['ipaddress']] = server_info

        # TODO: Similarly, this should be clean up or improve also.  For now
        #       just get the largest sriov_phys_ports value, this is needed
        #       for neutron server on controller nodes.
        largest_sriov_value = max([value.get('sriov_phys_ports', 0)
                                   for value in manifest_data.values()
                                   if isinstance(value, dict)])
        for md in manifest_data.keys():
            if (isinstance(manifest_data[md], dict) and
                    "control" in manifest_data[md].get('server_role', [])):
                manifest_data[md]['sriov_phys_ports'] = largest_sriov_value
        # fpga
        largest_fpga_value = max([value.get('fpga_phys_ports', 0)
                                  for value in manifest_data.values()
                                  if isinstance(value, dict)])
        for md in manifest_data.keys():
            if (isinstance(manifest_data[md], dict) and
                    "control" in manifest_data[md].get('server_role', [])):
                manifest_data[md]['fpga_phys_ports'] = largest_fpga_value

        #####################################################
        # Generate common variables.
        #####################################################
        all_dict = {}
        all_dict['vars'] = self.populate_ceph_data(manifest_data)

        pod_type = self.ymlhelper.get_pod_type()
        all_dict['vars']['PODTYPE'] = pod_type

        central_cvim_mon = self.ymlhelper.is_central_cvim_mon_enabled()
        all_dict['vars']['CENTRAL_CVIM_MON'] = central_cvim_mon

        inv_disc_enabled = self.ymlhelper.is_inventory_discovery_enabled()
        all_dict['vars']['CALIPSO_ENABLED'] = inv_disc_enabled

        vip_address = self.ymlhelper.get_external_vip_address()
        if vip_address:
            all_dict['vars']['external_lb_vip_address'] = vip_address
        else:
            if self.ymlhelper.get_pod_type() != "ceph":
                all_dict['vars']['external_lb_vip_address'] = \
                    manifest_data['haproxy_all']['hosts'][0]
        # IPv6
        vip_ipv6_address = self.ymlhelper.get_external_vip_ipv6_address()
        if vip_ipv6_address:
            all_dict['vars']['external_lb_vip_ipv6_address'] = vip_ipv6_address

        all_dict['vars']['KEYSTONE_API_VERSION'] = 3
        # Optional services
        optional_services = self.ymlhelper.get_setup_data_property(
            "OPTIONAL_SERVICE_LIST")
        if optional_services is not None:
            svc_name_prefix = "optional_service_"
            for svc in optional_services:
                if svc == "ceilometer":
                    all_dict['vars']["optional_service_ceilometer"] = True
                    all_dict['vars']['optional_service_gnocchi'] = True
                svc_name = svc_name_prefix + svc
                all_dict['vars'][svc_name] = True

        if self.ymlhelper.check_section_exists('NFVIMON'):
            all_dict['vars']['nfvimon'] = "True"
            all_dict['vars']["optional_service_ceilometer"] = True

        if self.ymlhelper.check_section_exists('LDAP'):
            all_dict['vars']['ldap'] = "True"

        if self.ymlhelper.check_section_exists('SWIFTSTACK'):
            all_dict['vars']['swift_service'] = True

        aci_infra_vlan = self.ymlhelper.get_segment_vlan_id('aciinfra')
        if aci_infra_vlan:
            all_dict['vars']['aci_infra_vlan'] = aci_infra_vlan

        if self.ymlhelper.get_mechanism_driver() == 'vts':
            all_dict['vars']['VTS_XRNC_TENANT_IPS'] = \
                self.get_vts_vtsr_underlay_ips()

        all_dict['vars']['install_dir'] = self._locate_install_dir()

        # Determine legacy storage network
        legacy_storage_net = self.use_legacy_storage_net()
        if legacy_storage_net:
            all_dict['vars']['legacy_storage_net'] = legacy_storage_net

        # Derive timezone
        all_dict['vars']['TIMEZONE'] = self.get_timezone()
        manifest_data['all'] = all_dict

        # Determine all supported intel card types across the cluster
        if self.ymlhelper.is_cisco_vic_intel_sriov():
            supported_intel_pids = self.ymlhelper.get_supported_vicnic_intel_pids()
        else:
            supported_intel_pids = list(set(pure_intel_pids))
        if supported_intel_pids:
            all_dict['vars']['supported_intel_pids'] = supported_intel_pids

        rma_tors = os.environ.get('REPLACE_TORS', None)
        if rma_tors:
            all_dict['vars']['rma_tors'] = rma_tors

        all_dict['vars']['ANSIBLE_SAFE_LOG_OVERRIDE'] = \
            self.get_ansible_safe_logging()
        all_dict['vars']['build_node_ip'] = self.get_build_node_ip('management')
        all_dict['vars']['build_node_registry'] = socket.gethostname()

        all_vlan_tuples = None
        if self.ymlhelper.get_tenant_vlan_range():
            tenant_tuples = \
                self.get_vlan_tuples(self.ymlhelper.get_tenant_vlan_range())
            all_dict['vars']['TENANT_VLAN_TUPLES'] = tenant_tuples
            all_vlan_tuples = tenant_tuples

        if self.ymlhelper.get_provider_vlan_range():
            provider_tuples = \
                self.get_vlan_tuples(self.ymlhelper.get_provider_vlan_range())
            all_dict['vars']['PROVIDER_VLAN_TUPLES'] = provider_tuples
            if all_vlan_tuples:
                all_vlan_tuples += ',' + provider_tuples
            else:
                all_vlan_tuples = provider_tuples

        if self.ymlhelper.get_mechanism_driver() == 'aci' \
                and all_vlan_tuples:
            all_dict['vars']['ALL_VLAN_TUPLES'] = all_vlan_tuples

        if self.ymlhelper.determine_ipa_info_delta():
            all_dict['vars']['ipa_delta'] = True

        if manifest_data:
            with open(gen_file, "w") as gen_f:
                gen_f.write(json.dumps(manifest_data, indent=4,
                                       sort_keys=True))

        return manifest_data

    def add_service_info_dict(self, current_service_info_dict,
                              new_service_info_dict):
        '''Add service info in dict'''
        current_service_info_list = []
        new_service_info_list = []
        if "hosts" in current_service_info_dict:
            current_service_info_list = current_service_info_dict["hosts"]
        if "hosts" in new_service_info_dict:
            new_service_info_list = new_service_info_dict["hosts"]

        added_service_info_dict = {}

        added_service_info_list = current_service_info_list + new_service_info_list
        added_service_info_list = list(set(added_service_info_list))

        if len(added_service_info_list) > 0:
            added_service_info_dict["hosts"] = added_service_info_list
        else:
            added_service_info_dict["hosts"] = []

        return added_service_info_dict

    def get_server_bonds(self, server):
        '''
        Returns server bonds if exists
        '''

        if self.bonds.get(server, None):
            return self.bonds[server]

        svr_dict = self.ymlhelper.get_server_info(server)
        if svr_dict and svr_dict.get("bonds", None):
            self.bonds[server] = svr_dict['bonds']
        else:
            try:
                cobbler_file = config_parser.YamlHelper(
                    user_input_file=common.get_cobbler_data_file_path())
                servers = cobbler_file.parsed_config.keys()
                for server_host in servers:
                    self.bonds[server_host] = \
                        cobbler_file.parsed_config[server_host]['bonds']
            except IOError:
                return None

        return self.bonds.get(server, None)

    def get_server_storage_type(self, mgmt_ip):
        '''
        Given management ip of server return whether the
        server is an HDD or SSD storage node
        '''
        try:
            server_hostname = None
            cobbler_file = config_parser.YamlHelper(
                user_input_file=common.get_cobbler_data_file_path())
            servers = cobbler_file.parsed_config.keys()
            for server_host in servers:
                if cobbler_file.parsed_config[server_host][\
                        'bonds']['management']['ipaddress'] == mgmt_ip:
                    server_hostname = server_host
                    break
            if server_hostname:
                return self.ymlhelper.get_ceph_cluster_info(server_hostname)
            else:
                return None
        except:
            return None

    def get_storage_ip_list(self):
        '''
        Returns ip addresses for all storage interfaces
        '''
        storage_ip_list = []
        try:
            cobbler_file = config_parser.YamlHelper(
                user_input_file=common.get_cobbler_data_file_path())
            if cobbler_file and cobbler_file.parsed_config:
                return [server['bonds']['storage']['ipaddress']
                        for server in cobbler_file.parsed_config.values()
                        if 'storage' in server['bonds'] and \
                        server.get('power_status') == 'on']

        except IOError:
            return None
        return storage_ip_list

    def get_cluster_ip_list(self):
        '''
        Returns ip addresses for all cluster/replication interfaces for ceph POD
        '''
        cluster_ip_list = []
        try:
            cobbler_file = config_parser.YamlHelper(
                user_input_file=common.get_cobbler_data_file_path())
            if cobbler_file and cobbler_file.parsed_config:
                return [server['bonds']['cluster']['ipaddress']
                        for server in cobbler_file.parsed_config.values()
                        if 'cluster' in server['bonds']]

        except IOError:
            return None
        return cluster_ip_list

    def netmask_to_cidr(self, netmask):
        '''
        Convert netmask to cidr
        '''
        return sum([bin(int(x)).count('1') for x in netmask.split('.')])

    def get_vlan_tuples(self, vlan_ranges):
        # TENANT_VLAN_RANGES: "2003:2500,2501:3200,3201:3500"
        # network_vlan_ranges = physnet1:2003:2500, physnet1:2501:3200, physnet1:3201:3500
        vlan_tuples = []
        for vlan_range in vlan_ranges.split(','):
            if len(vlan_range.split(':')) == 1:
                vlan_tuples.append('%s:%s' % (vlan_range, vlan_range))
            else:
                vlan_tuples.append(vlan_range)
        return ','.join(vlan_tuples)

    def populate_ceph_data(self, manifest_data):
        '''Populate Ceph data'''

        all_var_dic = {}
        all_var_dic['cinder_ceph_secret'] = self.get_cinder_secret_uuid()
        all_var_dic['ceph_cluster_id'] = ''
        all_var_dic['ceph_cinder_client_key'] = ''
        all_var_dic['ceph_cinder_backup_client_key'] = ''
        all_var_dic['ceph_glance_client_key'] = ''
        all_var_dic['ceph_gnocchi_client_key'] = ''
        all_var_dic['ceph_mon_hosts'] = ''
        all_var_dic['ceph_mon_members'] = ''
        cluster_config_file = common.get_absolute_path_for_file(
            __file__, CEPH_CLUSTER_ID_CONF)
        cluster_config_file = os.path.normpath(cluster_config_file)
        if os.path.exists(cluster_config_file):
            if self.ymlhelper.get_pod_type() != "ceph":
                ceph_mon_hosts = manifest_data['mon_hosts']['hosts']
            else:
                ceph_mon_hosts = manifest_data['ceph_mon_all']['hosts']
            if ceph_mon_hosts:
                all_var_dic['ceph_mon_hosts'] = ','.join(ceph_mon_hosts)
            if self.ymlhelper.get_pod_type() != "ceph":
                ceph_mon_members = self.ymlhelper.get_server_list(role='control')
            else:
                ceph_mon_members = self.ymlhelper.get_server_list(role='cephcontrol')
            if ceph_mon_members:
                all_var_dic['ceph_mon_members'] = ','.join(ceph_mon_members)
            id_file = open(cluster_config_file, 'r')
            cluster_id = id_file.readline().strip()
            all_var_dic['ceph_cluster_id'] = cluster_id
            id_file.close()

            base_path = os.path.dirname(cluster_config_file)
            ceph_path = os.path.join(base_path, cluster_id, 'etc/ceph')
            cinder_keyring_file = os.path.join(ceph_path, \
                                               'ceph.client.cinder.keyring')
            cinder_backup_keyring_file = os.path.join(\
                ceph_path, 'ceph.client.cinder-backup.keyring')
            glance_keyring_file = os.path.join(ceph_path, \
                                               'ceph.client.glance.keyring')
            self.log.debug("glance_keyring_file: %s" % glance_keyring_file)
            gnocchi_keyring_file = os.path.join(ceph_path,
                                                'ceph.client.gnocchi.keyring')
            if os.path.exists(cinder_keyring_file):
                cinder_key = self.get_keyring_key_value(cinder_keyring_file, \
                                                        'client.cinder')
                all_var_dic['ceph_cinder_client_key'] = cinder_key
            if os.path.exists(cinder_backup_keyring_file):
                cinder_backup_key = self.get_keyring_key_value(\
                    cinder_backup_keyring_file, 'client.cinder-backup')
                all_var_dic['ceph_cinder_backup_client_key'] = cinder_backup_key
            if os.path.exists(glance_keyring_file):
                glance_key = self.get_keyring_key_value(glance_keyring_file,
                                                        'client.glance')
                all_var_dic['ceph_glance_client_key'] = glance_key
            if os.path.exists(gnocchi_keyring_file):
                gnocchi_key = self.get_keyring_key_value(gnocchi_keyring_file,
                                                         'client.gnocchi')
                all_var_dic['ceph_gnocchi_client_key'] = gnocchi_key
        return all_var_dic


    def get_cinder_secret_uuid(self):
        '''Get Cinder secret UUID'''

        secret_uuid = str(uuid.uuid4())
        secret_path = os.path.expanduser('~/openstack-configs/')
        # Keep the id in memory if openstack-configs is not in home directory.
        if not os.path.exists(secret_path):
            return secret_uuid
        secret_file_name = os.path.join(secret_path, 'cinder-secret-uuid')
        if not os.path.exists(secret_file_name):
            # Save the uuid
            secret_file = open(secret_file_name, 'w+')
            secret_file.write(secret_uuid)
            secret_file.close()
        else:
            secret_file = open(secret_file_name, 'r')
            secret_uuid = secret_file.readline()
        return secret_uuid


    def get_keyring_key_value(self, keyring_file_path, section):
        '''Get keyring key value'''
        keyring_file = open(keyring_file_path)
        line_list = []
        for line in keyring_file:
            line_list.append(line.strip())
        keyring_data = '\n'.join(line_list)
        mem_file = StringIO.StringIO(keyring_data)
        keyring_file.close()

        config = ConfigParser.RawConfigParser()
        config.readfp(mem_file)
        return config.get(section, 'key')

    def get_ansible_safe_logging(self):
        return self.parsed_defaults.parsed_config.get(
            'ANSIBLE_SAFE_LOG_OVERRIDE', True)

    def append_openstack_manifest(self, gen_file=None):
        '''
        Append to an existing openstack manifest file.
        '''
        if gen_file is None:
            gen_file = "../system_configs/_openstack_manifest.yaml"

        if not os.path.exists(gen_file):
            self.log.error("Cannot append to manifest. Does not exist")
            return None

        with open(gen_file, 'rb') as gen_f:
            manifest_data = json.loads(gen_f.read())

        print "manifest: ", manifest_data

    def build_server_info_dict(self, server, allroles=False):
        '''
        Build the server dict.
        '''
        server_info = {}
        bonds = self.get_server_bonds(server)
        server_role = self.ymlhelper.get_server_cimc_role(server,
                                                          allroles=allroles)
        self.log.debug("Server role for server %s is %s",
                       server, server_role)

        server_info['server_role'] = server_role

        # server_info['ansible_host'] = bonds['api']['ipaddress']
        if bonds['management'].get('ipv6_address'):
            server_info['ansible_host'] = bonds['management']['ipv6_address']
        else:
            server_info['ansible_host'] = bonds['management']['ipaddress']
        # server_info['api_ip'] = bonds['api']['ipaddress']
        # server_info['control_ip'] = bonds['management']['ipaddress']

        return server_info

    def populate_bond_ip(self, svr, host, bond=None):
        '''
        Return the bond ip given the host and the type of
        bond ip required.
        '''
        bond_ip = None
        # Check if bond information exist for server
        bondinfo = self.get_server_bonds(host)
        if not bondinfo:
            return None

        if bond in ["control", "management"]:
            bond_ip = bondinfo['management']['ipaddress']
        if bond == "provision":
            bond_ip = bondinfo['provision']['ipaddress']
        if bond == "tenant":
            if bondinfo.get('tenant', None):
                bond_ip = bondinfo['tenant']['ipaddress']
        if bond == "api":
            if bondinfo.get('api', None):
                bond_ip = bondinfo.get('api').get('ipaddress', None)
        if bond == "storage":
            if bondinfo.get('storage', None):
                bond_ip = bondinfo['storage']['ipaddress']
        if bond == "cluster":
            if bondinfo.get("cluster", None):
                bond_ip = bondinfo['cluster']['ipaddress']

        return bond_ip

    def get_build_node_ip(self, segment, from_build_node="no"):
        '''
        Return the management node ip for the given segment
        '''

        # Return user specified management node IP if present in setup_data
        if re.search(r'no', from_build_node):
            build_ip = self.ymlhelper.nw_get_specific_vnic_info(segment, \
                                                                'build_node')
            if build_ip:
                return build_ip

        # Search interfaces of local node for IP address in subnet range
        subnet = None
        if (self.ymlhelper.get_data_from_userinput_file(
                ['internal_lb_vip_ipv6_address']) or
                self.ymlhelper.get_data_from_userinput_file(
                    ['PODTYPE']) == "ceph"):
            subnet = self.ymlhelper.nw_get_specific_vnic_info(segment,
                                                              "ipv6_subnet")
        if subnet is None:
            subnet = self.ymlhelper.nw_get_specific_vnic_info(segment, "subnet")

        # Override existing with remote management info if exists
        remote_mgmt = self.ymlhelper.nw_get_remote_management()
        if remote_mgmt:
            if remote_mgmt.get("ipv6_subnet"):
                subnet = remote_mgmt["ipv6_subnet"]
            elif remote_mgmt.get("subnet"):
                subnet = remote_mgmt["subnet"]

        cidr = netaddr.IPNetwork(subnet)
        af = netifaces.AF_INET6 if cidr.version == 6 else netifaces.AF_INET
        for interface in netifaces.interfaces():
            if af in netifaces.ifaddresses(interface):
                for address in netifaces.ifaddresses(interface)[af]:
                    if "%" in address['addr']:
                        continue
                    if netaddr.IPAddress(address['addr']) in cidr:
                        return address['addr']
        self.log.error("Cannot find local ip matching {0} subnet".format(segment))
        return None

    def get_fixed_services_hosts(self, servicename, service_count, host_list,
                                 bond=None):
        '''
        Return the service info dict for services
        which are started on specific hosts.
        '''
        service_info = {}

        service_info['hosts'] = []

        # Polulate the services info with the specific ip addr.
        # Default is mgmt_bond (API ip)
        if bond is None:
            bond = "control"

        if host_list is None:
            return service_info

        for host in host_list:
            if service_count == 0:
                break

            svr = self.ymlhelper.get_server_info(host)
            if svr.get('services', None) is not None:
                svc = [svc for svc in svr.get('services') if svc == servicename]
                if len(svc) > 0:
                    bond_ip = self.populate_bond_ip(svr, host, bond=bond)
                    if bond_ip:
                        service_info['hosts'].append(bond_ip)
                    service_count = service_count - 1

        return service_info

    def get_services_hosts(self, service_count, racklist, host_list,
                           role=None, bond=None):
        '''
        Return service info dict with hosts lists on which the
        services should be orchestrated.
        '''
        service_info = {}
        service_info['hosts'] = []

        if bond is None:
            bond = "control"

        self.log.debug("Get svc host: [%s] [%s] [%d] [%s]",
                       host_list, racklist, service_count, bond)

        if host_list is None:
            return service_info

        for host in host_list:
            if service_count == 0:
                break

            # Existence of 'fixed_svc_only' flag  in the server definition
            # means the user explicitly wants to do service placement
            # of specific services, specified by services list. So avoid
            # placing any services on those servers.
            svr = self.ymlhelper.get_server_info(host)
            forced_svc = svr.get('fixed_svc_only', False)
            if forced_svc:
                continue

            # Only check rack affinity for control services.
            if role == "control":
                rack_id = svr['rack_info']['rack_id']
                check = [rackid for rackid in racklist if rackid == rack_id]
                if len(check) == 0:
                    continue
                racklist.remove(rack_id)

            # check whether the user has given the input
            bond_ip = self.populate_bond_ip(svr, host, bond=bond)
            if bond_ip:
                service_info['hosts'].append(bond_ip)
            service_count = service_count - 1

        return service_info

    def get_all_services_hosts(self, host_list,
                               bond=None):
        '''
        Return service info dict with all the hosts as part of the hostlist.
        '''
        service_info = {}
        service_info['hosts'] = []

        if bond is None:
            bond = "control"

        self.log.debug("Get all svc host: [%s] [%s]", host_list, bond)

        if host_list is None:
            return service_info

        for host in host_list:
            svr = self.ymlhelper.get_server_info(host)
            # forced_svc = svr.get('fixed_svc_only', False)
            # if forced_svc:
            #    continue

            # check whether the user has given the input
            bond_ip = self.populate_bond_ip(svr, host, bond=bond)
            if bond_ip:
                service_info['hosts'].append(bond_ip)

        return service_info

    def get_vts_vtsr_underlay_ips(self):
        '''IP addresses supplied in setup_data will override the auto discovery'''

        vts_parameters = self.ymlhelper.get_setup_data_property('VTS_PARAMETERS')
        defined_vtsr_ips = sorted(vts_parameters['VTS_XRNC_TENANT_IPS']) \
            if 'VTS_XRNC_TENANT_IPS' in vts_parameters else None

        ip = vts_parameters['VTS_NCS_IP']
        u = vts_parameters['VTC_SSH_USERNAME']
        p = vts_parameters['VTC_SSH_PASSWORD']
        s = vts_parameters['VTS_SITE_UUID']
        try:
            client = VtcClient(ip, u, p, s)
            vtsr_ips = sorted(client.get_xrvr_underlay_ips())
        except Exception:
            vtsr_ips = defined_vtsr_ips

        return vtsr_ips \
            if not defined_vtsr_ips or defined_vtsr_ips == vtsr_ips else None

    def build_service_info_dict(self,
                                servicename,
                                role='control',
                                bond=None,
                                with_power_status=0):
        '''
        Build the service info dictionary:
        '''
        service_info = {}

        # Get the Hosts from pool of hosts to deploy the service on.
        host_list = []
        if role == "common" and not with_power_status:
            # Services with "common" role indicate they need to be on
            # control as well as compute nodes.
            control_host_list = self.ymlhelper.get_server_list(role="control")
            if control_host_list is None:
                control_host_list = []
            compute_host_list = self.ymlhelper.get_server_list(role="compute")
            if compute_host_list is None:
                compute_host_list = []
            host_list = [control_host_list, compute_host_list]
            host_list = list(set(list(itertools.chain(*host_list))))

        elif role == 'common' and with_power_status:
            try:
                control_host_list = self.ymlhelper.get_server_list(role="control")
                if control_host_list is None:
                    control_host_list = []
                compute_host_list = self.ymlhelper.get_server_list(role="compute")
                if compute_host_list is None:
                    compute_host_list = []
                common_host_list = [control_host_list, compute_host_list]
                common_host_list = list(set(list(itertools.chain(*common_host_list))))

                cobbler_file = config_parser.YamlHelper(
                    user_input_file=common.get_cobbler_data_file_path())
                servers = cobbler_file.parsed_config.keys()

                for server_host in servers:
                    if server_host in common_host_list and \
                            cobbler_file.parsed_config[server_host].get(\
                            'power_status') == 'on':
                        host_list.append(server_host)
            except IOError:
                self.log.info("Compute/Control info for {} not found "
                    "in cobbler file" \
                    .format(server_host))

        elif role == 'compute' and with_power_status:
            try:
                compute_host_list = self.ymlhelper.get_server_list(role="compute")
                cobbler_file = config_parser.YamlHelper(
                    user_input_file=common.get_cobbler_data_file_path())
                servers = cobbler_file.parsed_config.keys()
                for server_host in servers:
                    if server_host in compute_host_list and \
                            cobbler_file.parsed_config[server_host].get(\
                            'power_status') == 'on':
                        host_list.append(server_host)
                        if servicename == 'neutron_sriov_agent':
                            nic_sriov = cobbler_file.parsed_config[server_host].get(\
                                'create_sriov')
                            if not nic_sriov:
                                host_list.remove(server_host)

            except IOError:
                self.log.info("Compute info for {} not found in cobbler file" \
                             .format(server_host))

        elif role is None and with_power_status:
            try:
                available_servers = self.ymlhelper.get_server_list()
                cobbler_file = config_parser.YamlHelper(
                    user_input_file=common.get_cobbler_data_file_path())
                servers = cobbler_file.parsed_config.keys()
                for server_host in servers:
                    if server_host in available_servers and \
                            cobbler_file.parsed_config[server_host].get(\
                            'power_status') == 'on':
                        host_list.append(server_host)
            except IOError:
                self.log.info("Node info for {} not found in cobbler file" \
                             .format(server_host))
        else:
            host_list = self.ymlhelper.get_server_list(role=role)

        racklist = self.ymlhelper.get_server_unique_rack_list(role=role)
        docker_serviceinfo = self.ymlhelper.\
            dock_get_docker_service_info(servicename)
        if docker_serviceinfo is not None:
            service_count = docker_serviceinfo.get('service_count', 0)
        else:
            self.log.debug("Service info for %s not found", servicename)
            service_count = -1

        if service_count == -1:
            # A service count of -1 means that the service needs to be
            # run on any number of controllers, computes and storage nodes.
            service_count = DOCKER_MAX_SERVICE_COUNT

        self.log.debug("Build svcinfo dict [svc: %s svc-cnt: %d] [role: %s] "
                       "[bond: %s] [host_list: %s] [rack_list: %s]",
                       servicename, service_count, role,
                       bond, host_list, racklist)

        service_info = self.get_fixed_services_hosts(servicename,
                                                     service_count,
                                                     host_list,
                                                     bond=bond)
        hosts = service_info['hosts']
        if len(hosts) == 0:
            if role is None:
                service_info = self.get_all_services_hosts(host_list,
                                                           bond=bond)
            else:
                service_info = self.get_services_hosts(service_count,
                                                       racklist,
                                                       host_list,
                                                       role=role,
                                                       bond=bond)

        return service_info

    def get_torswitch_list(self):
        '''Get the list of tor switches'''

        tor_swt_details = self.ymlhelper.get_tor_switch_details()
        switch_list = []
        for item in tor_swt_details:
            if item.get('hostname') is not None:
                switch_list.append(item.get('hostname'))

        return switch_list

    def extend_auto_tor_to_aci_fabric(self):
        '''Check if Auto TOR needs to be extended
        to ACI without Plugin'''

        curr_mech_driver = self.ymlhelper.get_mechanism_driver()
        apic_info = self.ymlhelper.get_apic_info()

        if self.ymlhelper.get_pod_type() == 'ceph' and apic_info is not None:
            return 1
        elif curr_mech_driver == "openvswitch" and apic_info is not None:
            return 1
        elif curr_mech_driver == "vpp" and apic_info is not None:
            return 1

        return 0

    def is_cp_dp_collapsed(self):
        '''Check if control and data plance are collapsed
        Condition is true for VIC NIC or VIC or NIC with QCT'''

        if self.ymlhelper.is_cisco_vic_intel_sriov():
            return 1

        servers = self.ymlhelper.get_server_list()
        for server in servers:
            server_type = self.ymlhelper.get_platform_vendor(server)
            if server_type == 'QCT':
                return 1
        return 0

    def is_platform_qct(self):
        '''Check if platform has QCT'''

        servers = self.ymlhelper.get_server_list()
        for server in servers:
            server_type = self.ymlhelper.get_platform_vendor(server)
            if server_type == 'QCT':
                return 1
        return 0

    def fetch_apic_installer_tenant_info(self):
        '''Fetch apic installer tenant info if defined'''

        apic_inst_tenant = ['APICINFO', 'apic_installer_tenant']

        apic_tenant_info = \
            self.ymlhelper.get_data_from_userinput_file(apic_inst_tenant)

        if apic_tenant_info is None:
            return 0

        return apic_tenant_info

    def check_configure_aci_fabric(self):
        '''Fetch apic installer tenant info if defined'''

        cfg_fabric = ['APICINFO', 'configure_fabric']

        cfg_fabric_info = \
            self.ymlhelper.get_data_from_userinput_file(cfg_fabric)

        if cfg_fabric_info is None:
            return True

        return cfg_fabric_info

    def check_nfvbench_presence(self):
        '''Check if NFVBENCH is present in setup_data.yaml file'''

        key = ["NFVBENCH", "enabled"]
        ret_value = self.ymlhelper.get_data_from_userinput_file(key)
        if ret_value is not None and ret_value is True:
            return 1
        else:
            return 0

    def chk_for_item_in_server_section(self, item_name):
        '''Checks if item is defined in global/server section'''

        server_info = self.ymlhelper.get_data_from_userinput_file(['SERVERS'])
        for key, value in server_info.iteritems():
            item_chk = value.get(item_name, None)
            if item_chk is not None and item_chk >= 1:
                return 1

        return 0

    def is_vxlan_enabled(self):
        '''Check if VXLAN is enabled'''

        try:
            ntwrk_opt_info = \
                self.ymlhelper.get_data_from_userinput_file(['NETWORK_OPTIONS'])

            if not ntwrk_opt_info:
                return 0

            for key in ntwrk_opt_info.iterkeys():
                if key == 'vxlan':
                    return 1

        except AttributeError:
            return 0

        return 0

    def is_network_option_enabled(self, option_value):
        '''Check if L3VPN/vxlan/sr is enabled'''

        try:
            ntwrk_opt_info = \
                self.ymlhelper.get_data_from_userinput_file(['NETWORK_OPTIONS'])

            if not ntwrk_opt_info:
                return 0
            for key in ntwrk_opt_info.iterkeys():
                if key == option_value:
                    return 1
                elif key == 'vxlan':
                    vxlan_info = ntwrk_opt_info.get(key)
                    if vxlan_info is None:
                        return 0
                    for key1 in vxlan_info.iterkeys():
                        if key1 == option_value:
                            return 1
                elif key == 'sr-mpls':
                    mpls_info = ntwrk_opt_info.get(key)
                    if mpls_info is None:
                        return 0
                    for key1 in mpls_info.iterkeys():
                        if key1 == option_value:
                            return 1

        except AttributeError:
            return 0

        return 0

    def fetch_vlans_being_used(self):
        '''Fetch VLANs used'''

        curr_vlans_used = []
        int_lb_info = self.ymlhelper.get_data_from_userinput_file(\
            ['internal_lb_vip_ipv6_address'])
        via_v6 = 1

        if int_lb_info is None:
            int_lb_info = self.ymlhelper.get_data_from_userinput_file(\
                ['internal_lb_vip_address'])
            via_v6 = 0

        my_env, env_stat = common.generate_cloud_env(int_lb_info, via_v6=via_v6)

        if not env_stat:
            err_str = "ERROR: Can't read the openrc file to determine " \
                      "keystone auth tokens"
            self.log.info(err_str)
            return curr_vlans_used

        cred_dict = {}
        for key, value in my_env.iteritems():
            if key == 'OS_USERNAME':
                cred_dict['username'] = value

            elif key == 'OS_PROJECT_NAME':
                cred_dict['project_name'] = value

            elif key == 'OS_USER_DOMAIN_NAME':
                cred_dict['user_domain_name'] = value

            elif key == 'OS_AUTH_URL':
                cred_dict['auth_url'] = value

            elif key == 'OS_PASSWORD':
                cred_dict['password'] = value

            elif key == 'OS_PROJECT_DOMAIN_NAME':
                cred_dict['project_domain_name'] = value

        if not cred_dict:
            return curr_vlans_used

        nh = nclient.NeutronManage(cred_dict, 3)
        all_nets = nh.neutron_get_networks(external=False)

        if not all_nets:
            return curr_vlans_used

        for item in all_nets:
            for key, value in item.iteritems():
                if key == 'provider:segmentation_id' \
                        and item['provider:segmentation_id'] not in curr_vlans_used:
                    curr_vlans_used.append(item['provider:segmentation_id'])

        return curr_vlans_used

    @staticmethod
    def should_generate_inventory():
        """
        Method to find whether to generate inventory or not
        """
        generate = False
        home_dir = os.path.expanduser("~")
        setup_data = home_dir + "/openstack-configs/setup_data.yaml"
        cobbler_data = home_dir + "/openstack-configs/.cobbler_data.yaml"
        docker_yaml = home_dir + "/openstack-configs/docker.yaml"

        cluster_config_file = common.get_absolute_path_for_file(
            __file__, CEPH_CLUSTER_ID_CONF)
        cluster_config_file = os.path.normpath(cluster_config_file)

        keyring_file_list = []
        if os.path.exists(cluster_config_file):
            id_file = open(cluster_config_file, 'r')
            cluster_id = id_file.readline().strip()
            base_path = os.path.dirname(cluster_config_file)
            ceph_path = os.path.join(base_path, cluster_id, 'etc/ceph')
            cinder_keyring_file = os.path.join(ceph_path, \
                                               'ceph.client.cinder.keyring')
            cinder_backup_keyring_file = os.path.join(\
                ceph_path, 'ceph.client.cinder-backup.keyring')
            glance_keyring_file = os.path.join(ceph_path, \
                                               'ceph.client.glance.keyring')
            gnocchi_keyring_file = os.path.join(ceph_path,
                                                'ceph.client.gnocchi.keyring')
            keyring_file_list = [cinder_keyring_file,
                                 cinder_backup_keyring_file,
                                 glance_keyring_file, gnocchi_keyring_file]

        file_checksums = {}
        file_list = [setup_data, cobbler_data, docker_yaml]
        file_list.extend(keyring_file_list)
        check = None
        if os.path.isfile(".inventory_checksums.p"):
            try:
                check = json.load(open(".inventory_checksums.p", "rb"))
            except:
                # corrupted file
                try:
                    os.remove(".inventory_checksums.p")
                except OSError:
                    pass

        for f in file_list:
            if os.path.isfile(f):
                file_checksums[f] = \
                    hashlib.sha256(open(f, 'rb').read()).hexdigest()
            if check:
                if check.get(f, None) != file_checksums.get(f, None):
                    generate = True

        with open(".inventory_checksums.p", "w") as w:
            json.dump(file_checksums, w)

        return True

    def is_l3_fabric_enabled(self):
        '''Check if L3 Fabric is enabled
        Mulltiple checks: check for L3_PROVIDER_VNI_RANGES or
        networks/l3_fabric_vni or
        TORSWITCHINFO/SWITCHDETAILS/l3_fabric_loopback'''

        l3_prov_vni_range = \
            self.ymlhelper.get_data_from_userinput_file(['L3_PROVIDER_VNI_RANGES'])
        if l3_prov_vni_range is not None:
            return True

        networking_block = self.ymlhelper.nw_get_networking_blocks()
        for item in networking_block:
            if item.get('l3_fabric_vni', None) is not None:
                return True

        tor_switch_info = \
            self.ymlhelper.get_data_from_userinput_file(['TORSWITCHINFO'])
        if tor_switch_info is None:
            return False

        switch_details = ['TORSWITCHINFO', 'SWITCHDETAILS']
        switch_details_info = \
            self.ymlhelper.get_deepdata_from_userinput_file(switch_details)

        if switch_details_info is None:
            return False

        for item in switch_details_info:
            if item.get('l3_fabric_loopback', None) is not None:
                return True

        return False

    def is_vgpu_enabled(self):
        '''Check if VGPU is enabled'''

        compute_list = self.ymlhelper.get_server_list(role="compute")
        server_info = self.ymlhelper.get_data_from_userinput_file(['SERVERS'])
        for key, value in six.iteritems(server_info):
            if key in compute_list:
                hw_info = value.get('hardware_info', None)
                if hw_info is not None:
                    vgpu_info = hw_info.get('VGPU_TYPE', None)
                    if vgpu_info is not None:
                        return 1

        return 0

    def is_pod_targeted_with_ipv6(self):
        '''Check if IPv6 is enabled in setup_data'''

        ext_lb_v6 = \
            self.ymlhelper.get_data_from_userinput_file(['external_lb_vip_ipv6_address'])
        if ext_lb_v6 is not None:
            return True

        int_lb_v6 = \
            self.ymlhelper.get_data_from_userinput_file(['internal_lb_vip_ipv6_address'])
        if int_lb_v6 is not None:
            return True

        mgmt_v6_network_info = self.ymlhelper.nw_get_specific_vnic_info(
            'management', 'ipv6_subnet')
        if mgmt_v6_network_info is not None:
            return True

        api_v6_network_info = self.ymlhelper.nw_get_specific_vnic_info(
            'api', 'ipv6_subnet')
        if api_v6_network_info is not None:
            return True

        return False

    def fetch_vmtp_networks(self):
        """Fetch VMTP network info"""

        curr_subnet_list = []
        vmtp_info = self.ymlhelper.get_data_from_userinput_file(['VMTP_VALIDATION'])
        if vmtp_info is not None:
            vmtp_net_list = ['EXT_NET', 'PROV_NET']
            for item in vmtp_net_list:
                curr_net = ['VMTP_VALIDATION', item]
                cur_net_info = \
                    self.ymlhelper.get_deepdata_from_userinput_file(curr_net)
                if cur_net_info is not None:
                    curr_subnet = cur_net_info.get('NET_SUBNET', None)
                    curr_subnet_list.append(curr_subnet)
        return curr_subnet_list


def main():
    '''
    Config Manager main.
    '''
    print "Config Manager"


if __name__ == '__main__':
    main()
