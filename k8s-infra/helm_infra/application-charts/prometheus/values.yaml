rbac:
  create: true

imagePullSecrets:
# - name: "image-pull-secret"

## Define serviceAccount names for components. Defaults to component's fully qualified name.
##
serviceAccounts:
  alertmanager:
    create: true
    name:
  kubeStateMetrics:
    create: false
    name:
  nodeExporter:
    create: false
    name:
  pushgateway:
    create: false
    name:
  server:
    create: true
    name:

alertmanager:
  ## If false, alertmanager will not be installed
  ##
  enabled: true

  ## alertmanager container name
  ##
  name: alertmanager

  ## alertmanager container image
  ##
  image:
    repository: prom/alertmanager
    tag: "v0.15.3"
    pullPolicy: IfNotPresent

  ## alertmanager priorityClassName
  ##
  priorityClassName: ""

  ## Additional alertmanager container arguments
  ##
  extraArgs: {}

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  prefixURL: ""

  ## External URL which can access alertmanager
  ## Maybe same with Ingress host name
  baseURL: ""

  ## Additional alertmanager container environment variable
  ## For instance to add a http_proxy
  ##
  extraEnv: {}

  configMapOverrideName: ""

  ## The configuration file name to be loaded to alertmanager
  ## Must match the key within configuration loaded from ConfigMap/Secret
  ##
  configFileName: alertmanager.yml

  ingress:
    enabled: true
    annotations:
       kubernetes.io/ingress.class: nginx
       nginx.ingress.kubernetes.io/auth-type: basic
       nginx.ingress.kubernetes.io/auth-secret: prometheus-auth-france
       nginx.ingress.kubernetes.io/enable-cors: "true"
       nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required For Login'
       nginx.ingress.kubernetes.io/auth-tls-secret: "france/cvimmon-alertmanager-france"
       nginx.ingress.kubernetes.io/rewrite-target: /
       nginx.ingress.kubernetes.io/add-base-url: "true"

    ## alertmanager Ingress additional labels
    ##
    extraLabels: {}

    ## alertmanager Ingress hostnames with optional path
    ## Must be provided if Ingress is enabled
    ##
    hosts:
      - cvimmon-alertmanager-france.cisco.com
    #   - alertmanager.domain.com
    #   - domain.com/alertmanager

    ## alertmanager Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls:
       - secretName: cvimmon-alertmanager-france
         hosts:
            - cvimmon-alertmanager-france.cisco.com

  ## Alertmanager Deployment Strategy type
  strategy:
    type: RollingUpdate

  ## Node tolerations for alertmanager scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for alertmanager pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Pod affinity
  ##
  affinity: {}

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  persistentVolume:
    ## If true, alertmanager will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## alertmanager data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteMany

    ## alertmanager data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## alertmanager data Persistent Volume existing claim name
    ## Requires alertmanager.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## alertmanager data Persistent Volume mount root path
    ##
    mountPath: /data

    ## alertmanager data Persistent Volume size
    ##
    size: 50Gi

    ## alertmanager data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: "portworx-sc"

    ## Subdirectory of alertmanager data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

  ## Annotations to be added to alertmanager pods
  ##
  podAnnotations: {}

  replicaCount: 1

  statefulSet:
    ## If true, use a statefulset instead of a deployment for pod management.
    ## This allows to scale replicas to more than 1 pod
    ##
    enabled: false

    podManagementPolicy: OrderedReady

    ## Alertmanager headless service to use for the statefulset
    ##
    headless:
      annotations: {}
      labels: {}

      ## Enabling peer mesh service end points for enabling the HA alert manager
      ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md
      # enableMeshPeer : true

      servicePort: 80

  ## alertmanager resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 10m
    #   memory: 32Mi
    # requests:
    #   cpu: 10m
    #   memory: 32Mi

  ## Security context to be added to alertmanager pods
  ##
  securityContext: {}

  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## Enabling peer mesh service end points for enabling the HA alert manager
    ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md
    # enableMeshPeer : true

    ## List of IP addresses at which the alertmanager service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    # nodePort: 30000
    type: ClusterIP

## Monitors ConfigMap changes and POSTs to a URL
## Ref: https://github.com/jimmidyson/configmap-reload
##
configmapReload:
  ## configmap-reload container name
  ##
  name: configmap-reload

  ## configmap-reload container image
  ##
  image:
    repository: jimmidyson/configmap-reload
    tag: "v0.2.2"
    pullPolicy: IfNotPresent

  ## Additional configmap-reload container arguments
  ##
  extraArgs: {}
  ## Additional configmap-reload volume directories
  ##
  extraVolumeDirs: []


  ## Additional configmap-reload mounts
  ##
  extraConfigmapMounts: []
    # - name: prometheus-alerts
    #   mountPath: /etc/alerts.d
    #   subPath: ""
    #   configMap: prometheus-alerts
    #   readOnly: true


  ## configmap-reload resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}

initChownData:
  ## If false, data ownership will not be reset at startup
  ## This allows the prometheus-server to be run with an arbitrary user
  ##
  enabled: true

  ## initChownData container name
  ##
  name: init-chown-data

  ## initChownData container image
  ##
  image:
    repository: busybox
    tag: "1.30"
    pullPolicy: IfNotPresent

  ## initChownData resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}

kubeStateMetrics:
  ## If false, kube-state-metrics will not be installed
  ##
  enabled: false

  ## kube-state-metrics container name
  ##
  name: kube-state-metrics

  ## kube-state-metrics container image
  ##
  image:
    repository: quay.io/coreos/kube-state-metrics
    tag: "v1.5.0"
    pullPolicy: IfNotPresent

  ## kube-state-metrics priorityClassName
  ##
  priorityClassName: ""

  ## kube-state-metrics container arguments
  ##
  args: {}

  ## Node tolerations for kube-state-metrics scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for kube-state-metrics pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to kube-state-metrics pods
  ##
  podAnnotations: {}

  pod:
    labels: {}

  replicaCount: 1

  ## kube-state-metrics resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 10m
    #   memory: 16Mi
    # requests:
    #   cpu: 10m
    #   memory: 16Mi

  ## Security context to be added to kube-state-metrics pods
  ##
  securityContext: {}

  service:
    annotations:
      prometheus.io/scrape: "true"
    labels: {}

    # Exposed as a headless service:
    # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
    clusterIP: None

    ## List of IP addresses at which the kube-state-metrics service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    type: ClusterIP

nodeExporter:
  ## If false, node-exporter will not be installed
  ##
  enabled: false

  ## If true, node-exporter pods share the host network namespace
  ##
  hostNetwork: true

  ## If true, node-exporter pods share the host PID namespace
  ##
  hostPID: true

  ## node-exporter container name
  ##
  name: node-exporter

  ## node-exporter container image
  ##
  image:
    repository: prom/node-exporter
    tag: "v0.17.0"
    pullPolicy: IfNotPresent

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    enabled: False
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  ## node-exporter priorityClassName
  ##
  priorityClassName: ""

  ## Custom Update Strategy
  ##
  updateStrategy:
    type: OnDelete

  ## Additional node-exporter container arguments
  ##
  extraArgs: {}

  ## Additional node-exporter hostPath mounts
  ##
  extraHostPathMounts: []
    # - name: textfile-dir
    #   mountPath: /srv/txt_collector
    #   hostPath: /var/lib/node-exporter
    #   readOnly: true

  extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /prometheus
    #   configMap: certs-configmap
    #   readOnly: true

  ## Node tolerations for node-exporter scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for node-exporter pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to node-exporter pods
  ##
  podAnnotations: {}

  ## Labels to be added to node-exporter pods
  ##
  pod:
    labels: {}

  ## node-exporter resource limits & requests
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 200m
    #   memory: 50Mi
    # requests:
    #   cpu: 100m
    #   memory: 30Mi

  ## Security context to be added to node-exporter pods
  ##
  securityContext: {}
    # runAsUser: 0

  service:
    annotations:
      prometheus.io/scrape: "true"
    labels: {}

    # Exposed as a headless service:
    # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
    clusterIP: None

    ## List of IP addresses at which the node-exporter service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    hostPort: 9100
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9100
    type: ClusterIP

server:
  ## Prometheus server container name
  ##
  name: server

  ## Prometheus server container image
  ##
  image:
    repository: prom/prometheus
    tag: "v2.9.0"
    pullPolicy: IfNotPresent

  ## prometheus server priorityClassName
  ##
  priorityClassName: ""

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  prefixURL: ""

  ## External URL which can access alertmanager
  ## Maybe same with Ingress host name
  baseURL: ""

  ## This flag controls access to the administrative HTTP API which includes functionality such as deleting time
  ## series. This is disabled by default.
  enableAdminApi: false

  ## This flag controls BD locking
  skipTSDBLock: false

  ## Path to a configuration file on prometheus server container FS
  configPath: /etc/config/prometheus.yml

  global:
    ## How frequently to scrape targets by default
    ##
    scrape_interval: 15s
    ## How long until a scrape request times out
    ##
    scrape_timeout: 10s
    ## How frequently to evaluate rules
    ##
    evaluation_interval: 15s

  ## Additional Prometheus server container arguments
  ##
  extraArgs: {}

  ## Additional Prometheus server hostPath mounts
  ##
  extraHostPathMounts: []
    # - name: certs-dir
    #   mountPath: /etc/kubernetes/certs
    #   subPath: ""
    #   hostPath: /etc/kubernetes/certs
    #   readOnly: true

  extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /prometheus
    #   subPath: ""
    #   configMap: certs-configmap
    #   readOnly: true

  ## Additional Prometheus server Secret mounts
  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.
  extraSecretMounts: []
    # - name: secret-files
    #   mountPath: /etc/secrets
    #   subPath: ""
    #   secretName: prom-secret-files
    #   readOnly: true

  configMapOverrideName: ""

  ingress:
    ## If true, Prometheus server Ingress will be created
    ##
    enabled: true

    ## Prometheus server Ingress annotations
    ##
    annotations:
       kubernetes.io/ingress.class: nginx
       nginx.ingress.kubernetes.io/auth-type: basic
       nginx.ingress.kubernetes.io/auth-secret: prometheus-auth-france
       nginx.ingress.kubernetes.io/enable-cors: "true"
       nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required For Login'
       nginx.ingress.kubernetes.io/auth-tls-secret: "france/cvimmon-prometheus-france"
       nginx.ingress.kubernetes.io/rewrite-target: /
       nginx.ingress.kubernetes.io/add-base-url: "true"

    ## Prometheus server Ingress additional labels
    ##
    extraLabels: {}

    ## Prometheus server Ingress hostnames with optional path
    ## Must be provided if Ingress is enabled
    ##
    hosts:
      - cvimmon-prometheus-france.cisco.com

    ## Prometheus server Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls:
       - secretName: cvimmon-prometheus-france
         hosts:
           - cvimmon-prometheus-france.cisco.com

  ## Server Deployment Strategy type
  strategy:
    type: RollingUpdate

  ## Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for Prometheus server pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Pod affinity
  ##
  affinity: {}

  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:

  persistentVolume:
    ## If true, Prometheus server will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Prometheus server data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Prometheus server data Persistent Volume annotations
    ##
    annotations: {}

    ## Prometheus server data Persistent Volume existing claim name
    ## Requires server.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## Prometheus server data Persistent Volume mount root path
    ##
    mountPath: /data

    ## Prometheus server data Persistent Volume size
    ##
    size: 50Gi

    ## Prometheus server data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: "portworx-sc"

    ## Subdirectory of Prometheus server data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

  ## Annotations to be added to Prometheus server pods
  ##
  podAnnotations: {}
    # iam.amazonaws.com/role: prometheus

  replicaCount: 1

  statefulSet:
    ## If true, use a statefulset instead of a deployment for pod management.
    ## This allows to scale replicas to more than 1 pod
    ##
    enabled: false

    annotations: {}
    podManagementPolicy: OrderedReady

    ## Alertmanager headless service to use for the statefulset
    ##
    headless:
      annotations: {}
      labels: {}
      servicePort: 80

  ## Prometheus server resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 500m
    #   memory: 512Mi
    # requests:
    #   cpu: 500m
    #   memory: 512Mi

  ## Security context to be added to server pods
  ##
  securityContext: {}

  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    type: ClusterIP

  ## Prometheus server pod termination grace period
  ##
  terminationGracePeriodSeconds: 300

  ## Prometheus data retention period (i.e 360h)
  ##
  retention: ""

pushgateway:
  ## If false, pushgateway will not be installed
  ##
  enabled: false

  ## pushgateway container name
  ##
  name: pushgateway

  ## pushgateway container image
  ##
  image:
    repository: prom/pushgateway
    tag: "v0.6.0"
    pullPolicy: IfNotPresent

  ## pushgateway priorityClassName
  ##
  priorityClassName: ""

  ## Additional pushgateway container arguments
  ##
  extraArgs: {}

  ingress:
    ## If true, pushgateway Ingress will be created
    ##
    enabled: false

    ## pushgateway Ingress annotations
    ##
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## pushgateway Ingress hostnames with optional path
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - pushgateway.domain.com
    #   - domain.com/pushgateway

    ## pushgateway Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-alerts-tls
    #     hosts:
    #       - pushgateway.domain.com

  ## Node tolerations for pushgateway scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for pushgateway pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to pushgateway pods
  ##
  podAnnotations: {}

  replicaCount: 1

  ## pushgateway resource requests and limits
  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 10m
    #   memory: 32Mi
    # requests:
    #   cpu: 10m
    #   memory: 32Mi

  ## Security context to be added to push-gateway pods
  ##
  securityContext: {}

  service:
    annotations:
      prometheus.io/probe: pushgateway
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the pushgateway service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9091
    type: ClusterIP

## alertmanager ConfigMap entries
##
alertmanagerFiles:
  alertmanager.yml:
    global:
      resolve_timeout: 5m

    receivers:
      - name: 'snmp'
        webhook_configs:
        - send_resolved: true
          url: 'http://localhost:1161/alarms'
        # slack_configs:
        #  - channel: '@you'
        #    send_resolved: true

    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 8737h
      # A default receiver
      receiver: snmp

## Prometheus server ConfigMap entries
##
serverFiles:

  ## Alerts configuration
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
  alerting_rules.yml:
     groups:

      -   name: instance
          rules:
          -   alert: instance_down
              annotations:
                  description: '{{ $labels.instance }} instance is down, check
                      Prometheus->Status->Targets={{ $labels.job }}'
                  summary: Instance down
              expr: up == 0
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: instance_down
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: ucs_monitor
          rules:
          -   alert: ucs_server_fault
              annotations:
                  description: '{{ $labels.description }}'
                  summary: '{{ $labels.summary }}'
              expr: cvim_ext_alert == 1
              labels:
                  severity: '{{ $labels.severity }}'
                  snmp_fault_code: '{{ $labels.snmp_fault_code }}'
                  snmp_fault_severity: '{{ $labels.snmp_fault_severity }}'
                  snmp_fault_source: '{{ $labels.snmp_fault_source }}'
                  snmp_node: '{{ $labels.snmp_node }}'
                  snmp_podid: "@@PODNAME@@"
      -   name: disk
          rules:
          -   alert: disk_used_percent
              annotations:
                  description: '{{ $labels.instance }} disk_used_percent device:{{ $labels.device
                      }} path:{{ $labels.path }} is {{ $value | printf "%.2f" }}%'
                  summary: Disk used > 90%
              expr: disk_used_percent > 90
              for: 5m
              labels:
                  severity: warning
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: alert
                  snmp_fault_source: disk_used_percent
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: disk_filling_up_in_4h
              annotations:
                  description: '{{ $labels.instance }} device:{{ $labels.device }} path:{{
                      $labels.path }} will fill up within 4 hours'
                  summary: Disk will fill up within 4 hours
              expr: (predict_linear(disk_free[1h], 4*3600) < 0) AND (system_uptime > 7200)
              for: 5m
              labels:
                  severity: critical
                  snmp_fault_code: resourceUsage
                  snmp_fault_severity: major
                  snmp_fault_source: disk_filling_up_in_4h
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: docker
          rules:
          -   alert: docker_container_down
              annotations:
                  description: '{{ $labels.instance }} container {{ $labels.container_name }} on
                      {{ $labels.host }} is down'
                  summary: Container down
              expr: docker_container_status_exitcode{container_status!~"running|created"}
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: docker_container_status_exitcode
                  snmp_node: '{{ $labels.host }}'
                  snmp_podid: "@@PODNAME@@"
      -   name: linkstate
          rules:
          -   alert: link_down_lacp
              annotations:
                  description: '{{ $labels.instance }} LACP link {{ $labels.physical_interface }}
                      status is {{ $value | printf "%.0f" }}'
                  summary: Link in LACP is down
              expr: linkstate_actor != 61 or linkstate_partner != 61
              for: 30s
              labels:
                  severity: warning
                  snmp_fault_code: hardwareFailure
                  snmp_fault_severity: alert
                  snmp_fault_source: linkstate_actor, linkstate_partner
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: link_down_sriov
              annotations:
                  description: '{{ $labels.instance }} SRIOV link {{ $labels.interface }} is down'
                  summary: SRIOV link is down
              expr: linkstate_sriov != 1
              for: 30s
              labels:
                  severity: warning
                  snmp_fault_code: hardwareFailure
                  snmp_fault_severity: alert
                  snmp_fault_source: linkstate_sriov
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: memory
          rules:
          -   alert: mem_available_percent
              annotations:
                  description: '{{ $labels.instance }} available memory is low
                      ({{ $value | printf "%.2f" }}% < 10%)'
                  summary: Low available memory < 10%
              expr: (sum by (host) (mem_free{host="$host"}) + sum by (host) (mem_buffered{host="$host"})
                  + sum by (host) (mem_cached{host="$host"})) / (sum by (host) (mem_total{host="$host"})
                  - sum by (host) (hugepages_nr{host="$host"})) * 100 < 10
              for: 5m
              labels:
                  severity: informational
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: informational
                  snmp_fault_source: mem_free_percent_4kb
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: memory_running_out_in_4h
              annotations:
                  description: '{{ $labels.instance }} memory may run out within 4 hours'
                  summary: Memory may run out within 4 hours
              expr: (predict_linear(mem_available[1h], 4*3600) < 0) AND (system_uptime > 7200)
              for: 5m
              labels:
                  severity: critical
                  snmp_fault_code: resourceUsage
                  snmp_fault_severity: major
                  snmp_fault_source: memory_running_out_in_4h
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: swap
          rules:
          -   alert: swap_used_percent
              annotations:
                  description: '{{ $labels.instance }} swap_used_percent is {{ $value | humanize1024 }}'
                  summary: Swap used is > 80%
              expr: swap_used_percent > 80
              for: 1m
              labels:
                  severity: warning
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: alert
                  snmp_fault_source: swap_used_percent
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: conntrack
          rules:
          -   alert: conntrack_percent
              annotations:
                  description: '{{ $labels.instance }} conntrack_ip_conntrack_count is >80% of max'
                  summary: conntrack_ip_conntrack_count is >80% of max
              expr: (conntrack_ip_conntrack_count / conntrack_ip_conntrack_max) > 0.8
              for: 1m
              labels:
                  severity: warning
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: alert
                  snmp_fault_source: conntrack_percent
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: system_uptime
          rules:
          -   alert: reboot
              annotations:
                  description: '{{ $labels.instance }} system_uptime is {{ $value }}s, indicating
                      a reboot'
                  summary: Server rebooted
              expr: system_uptime < 600
              for: 1m
              labels:
                  severity: warning
                  snmp_fault_code: hardwareFailure
                  snmp_fault_severity: alert
                  snmp_fault_source: reboot
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: users
          rules:
          -   alert: system_n_users
              annotations:
                  description: '{{ $labels.instance }} number of logged in users is {{ $value }}'
                  summary: High amount of users logged in
              expr: system_n_users > 10
              for: 1m
              labels:
                  severity: warning
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: alert
                  snmp_fault_source: system_n_users
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: ceph
          rules:
          -   alert: ceph_osdmap_num_in_osds
              annotations:
                  description: '{{ $labels.instance }} some ceph OSDs are not IN, check
                      ceph osd tree'
                  summary: Ceph OSDs not IN
              expr: ceph_osdmap_num_in_osds < ceph_osdmap_num_osds
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: major
                  snmp_fault_source: ceph_osdmap_num_in_osds
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: ceph_osdmap_num_up_osds
              annotations:
                  description: '{{ $labels.instance }} some ceph OSDs are not UP, check
                      ceph osd tree'
                  summary: Ceph OSDs not UP
              expr: ceph_osdmap_num_up_osds < ceph_osdmap_num_osds
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: major
                  snmp_fault_source: ceph_osdmap_num_up_osds
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: ceph_pgmap_state_count
              annotations:
                  description: '{{ $labels.instance }} ceph PG Map State not all active+clean,
                      check ceph status -w'
                  summary: Ceph PG map state not all active+clean
              expr: sum(ceph_pgmap_state_count{state!="active+clean"}) > 0
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: resourceUsage
                  snmp_fault_severity: major
                  snmp_fault_source: ceph_pgmap_state_count
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: ceph_pgmap_bytes_avail_filling_up_in_4h
              annotations:
                  description: '{{ $labels.instance }} ceph_pgmap_bytes_avail will drop
                      to zero within 4 hours.'
                  summary: Ceph will run out of space within 4 hours
              expr: (predict_linear(ceph_pgmap_bytes_avail[1h], 4*3600) < 0) AND (system_uptime > 7200)
              for: 5m
              labels:
                  severity: critical
                  snmp_fault_code: resourceUsage
                  snmp_fault_severity: major
                  snmp_fault_source: ceph_pgmap_bytes_avail_filling_up_in_4h
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: ceph_pgmap_bytes_used_percent
              annotations:
                  description: '{{ $labels.instance }} ceph_pgmap_bytes_used percent is
                      $value | printf "%.2f" }}%'
                  summary: Ceph PG map bytes used > 70%
              expr: (ceph_pgmap_bytes_used / ceph_pgmap_bytes_total) > .7
              for: 5m
              labels:
                  severity: warning
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: alert
                  snmp_fault_source: ceph_pgmap_bytes_used_percent
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: ceph_pgmap_bytes_used_percent
              annotations:
                  description: '{{ $labels.instance }} ceph_pgmap_bytes_used percent is
                      $value | printf "%.2f" }}%'
                  summary: Ceph PG map bytes used > 80%
              expr: (ceph_pgmap_bytes_used / ceph_pgmap_bytes_total) > .8
              for: 5m
              labels:
                  severity: critical
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: major
                  snmp_fault_source: ceph_pgmap_bytes_used_percent
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: haproxy
          rules:
          -   alert: haproxy_plugin_data_absent
              annotations:
                  description: '{{ $labels.instance }} HAProxy Telegraf plugin not returning data'
                  summary: HAProxy telegraf plugin not returning data
              expr: absent(haproxy_active_servers)
              for: 10m
              labels:
                  severity: informational
                  snmp_fault_code: other
                  snmp_fault_severity: informational
                  snmp_fault_source: haproxy_plugin_data_absent
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: haproxy_multiple_masters
              annotations:
                  description: '{{ $labels.host }} HAProxy Reporting multiple masters'
                  summary: HAProxy Reporting multiple masters
              expr: count(haproxy_active_servers{sv="galera-active1"}) by (region,metro,job) > 1
              for: 8m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: critical
                  snmp_fault_source: haproxy_active_servers
                  snmp_node: '{{ $labels.host }}'
                  snmp_podid: "@@PODNAME@@"
          -   alert: haproxy_active_servers_down
              annotations:
                  description: '{{ $labels.instance }} HAProxy active server status not UP'
                  summary: HAProxy active server status not UP
              expr: haproxy_active_servers{status!="UP"} > 0
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: haproxy_active_servers_down
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: haproxy_active_servers_backend
              annotations:
                  description: '{{ $labels.instance }} HAProxy active server backends should
                      be 3 but is {{ $value }}'
                  summary: HAProxy backend servers missing or down
              expr: haproxy_active_servers{status="UP", type="backend",
                  proxy!="galera_cluster-internal_vip", proxy!="stats"} != 3
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: haproxy_active_servers_backend
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: haproxy_active_servers_galera
              annotations:
                  description: '{{ $labels.instance }} HAProxy galera_cluster-internal_vip
                      active should be singular but is {{ $value }}'
                  summary: HAProxy galera active servers are not singular
              expr: haproxy_active_servers{proxy="galera_cluster-internal_vip", type="backend"} != 1
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: critical
                  snmp_fault_source: haproxy_active_servers_galera
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: haproxy_backup_servers_galera
              annotations:
                  description: '{{ $labels.instance }} HAProxy galera_cluster-internal_vip
                      backup should be 2 for quorum but is {{ $value }}'
                  summary: HAProxy galera does not have 2 backup servers
              expr: haproxy_backup_servers{proxy="galera_cluster-internal_vip",type="backend"} != 2
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: haproxy_backup_servers_galera
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: http_response
          rules:
          -   alert: http_service_unavailable
              annotations:
                  description: '{{ $labels.instance }} HTTP service {{ $labels.service }} ({{ $labels.server }}) is not responding'
                  summary: HTTP service is not available
              expr: http_response_result_code != 0
              for: 5m
              labels:
                  severity: warning
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: alert
                  snmp_fault_source: http_response_result_code
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: rabbitmq
          rules:
          -   alert: rabbitmq_node_running
              annotations:
                  description: '{{ $labels.instance }} one or more rabbitmq nodes are not running'
                  summary: Rabbitmq nodes not running
              expr: (sum(rabbitmq_node_running) / 3) != 3
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: rabbitmq_node_running
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: rabbitmq_node_mem_used_percent
              annotations:
                  description: '{{ $labels.instance }} rabbitmq node {{ $labels.node }}
                      memory usage is $value | printf "%.2f" }}%'
                  summary: Rabbitmq node memory usage at 90%
              expr: (rabbitmq_node_mem_used / rabbitmq_node_mem_limit) > .9
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: major
                  snmp_fault_source: rabbitmq_node_mem_used_percent
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: rabbitmq_queue_consumers
              annotations:
                  description: '{{ $labels.instance }} Rabbitmq node {{ $labels.node }}
                      has 0 consumers for queue {{ $labels.queue }}'
                  summary: Rabbitmq queue has no consumers
              expr: rabbitmq_queue_consumers{queue!~".*notif.*", queue!~".*engine.*",
                  queue!~".*trunk.*", queue!~".*fanout.*", queue!~".*reply.*",
                  queue!~".*server-resource.*"} == 0
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: major
                  snmp_fault_source: rabbitmq_queue_consumers
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
          -   alert: rabbitmq_queue_messages
              annotations:
                  description: '{{ $labels.instance }} Rabbitmq queued ready/unacked
                      message total is too high: {{ $value }}'
                  summary: Rabbitmq queued ready/unacked messages too high
              expr: sum(rabbitmq_queue_messages_unack) > 300 or sum(rabbitmq_queue_messages_ready) > 300
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: resourceUsage
                  snmp_fault_severity: major
                  snmp_fault_source: rabbitmq_queue_messages_unack, rabbitmq_queue_messages_ready
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: ntp
          rules:
          -   alert: ntpq_offset
              annotations:
                  description: '{{ $labels.instance }} ntpq_offset is {{ $value }}'
                  summary: NTP time out of sync
              expr: (ntpq_offset < -2500) OR (ntpq_offset > 2500)
              for: 1m
              labels:
                  severity: warning
                  snmp_fault_code: resourceThreshold
                  snmp_fault_severity: alert
                  snmp_fault_source: ntpq_offset
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: memcached
          rules:
          -   alert: memcached_restarted
              annotations:
                  description: '{{ $labels.instance }} memcached restarted, uptime is
                      {{ $value }}s'
                  summary: Memcached restarted
              expr: memcached_uptime < 180
              for: 1m
              labels:
                  severity: warning
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: alert
                  snmp_fault_source: memcached_restarted
                  snmp_node: "{{ $labels.instance }}"
                  snmp_podid: "@@PODNAME@@"
      -   name: openstack_service_status
          rules:
          -   alert: cp_galera_down
              annotations:
                  description: "Galera service is down on host {{ $labels.host }}"
                  summary: Galera service down
              expr: cp_galera_server_up == 0
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: cp_galera
                  snmp_node: '{{ $labels.host }}'
                  snmp_podid: "@@PODNAME@@"
          -   alert: cp_openstack_service_down
              annotations:
                  description: "Openstack service {{ $labels.name }} is down"
                  summary: Openstack service down
              expr: cp_openstack_service_up == 0
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: cp_openstack_service
                  snmp_node: '{{ $labels.name }}'
                  snmp_podid: "@@PODNAME@@"
          -   alert: cp_rabbitmq_down
              annotations:
                  description: "RabbitMQ service is down on host {{ $labels.host }}"
                  summary: RabbitMQ service down
              expr: cp_rabbitmq_server_up == 0
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: cp_rabbitmq
                  snmp_node: '{{ $labels.host }}'
                  snmp_podid: "@@PODNAME@@"
          -   alert: cp_ceph_error
              annotations:
                  description: "CEPH is in error state"
                  summary: CEPH in error state
              expr: cp_ceph_health == 2
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: cp_ceph
                  snmp_podid: "@@PODNAME@@"
          -   alert: cp_hypervisor_down
              annotations:
                  description: "Hypervisor is down on host {{ $labels.host }}"
                  summary: Hypervisor down
              expr: cp_hypervisor_up == 0
              for: 1m
              labels:
                  severity: critical
                  snmp_fault_code: serviceFailure
                  snmp_fault_severity: major
                  snmp_fault_source: cp_hypervisor
                  snmp_podid: "@@PODNAME@@"
      -   name: x509_cert
          rules:
          -   alert: certificate_expiring_5d
              annotations:
                  description: '{{ $labels.instance }}: Certificate {{ $labels.source }}
                      is expiring in less than 5 days'
                  summary: Certificate expiring in less than 5 days
              expr: min_over_time(x509_cert_expiry[12h]) <= 432000
              labels:
                  severity: critical
                  snmp_fault_code: other
                  snmp_fault_severity: critical
                  snmp_fault_source: x509_cert_expiry
                  snmp_node: '{{ $labels.host }}'
                  snmp_podid: "@@PODNAME@@"
          -   alert: certificate_expiring_10d
              annotations:
                  description: '{{ $labels.instance }}: Certificate {{ $labels.source }}
                      is expiring in less than 10 days'
                  summary: Certificate expiring in less than 10 days
              expr: min_over_time(x509_cert_expiry[12h]) <= 864000
              labels:
                  severity: warning
                  snmp_fault_code: other
                  snmp_fault_severity: major
                  snmp_fault_source: x509_cert_expiry
                  snmp_node: '{{ $labels.host }}'
                  snmp_podid: "@@PODNAME@@"
          -   alert: certificate_expiring_45d
              annotations:
                  description: '{{ $labels.instance }}: Certificate {{ $labels.source }}
                      is expiring in less than 45 days'
                  summary: Certificate expiring in less than 45 days
              expr: min_over_time(x509_cert_expiry[12h]) <= 3888000
              labels:
                  severity: informational
                  snmp_fault_code: other
                  snmp_fault_severity: informational
                  snmp_fault_source: x509_cert_expiry
                  snmp_node: '{{ $labels.host }}'
                  snmp_podid: "@@PODNAME@@"

  prometheus.yml:
    rule_files:
      - /etc/config/alerting_rules.yml
    scrape_configs:
      - basic_auth:
          password: 51feddea8ef0d1e9612e
          username: admin
        job_name: magnolia
        metrics_path: /metrics
        scheme: https
        scrape_interval: 10s
        scrape_timeout: 10s
        static_configs:
        - targets:
          - 172.28.123.143:9283
        tls_config:
          ca_file: /data/certs/magnolia-haproxy.crt
networkPolicy:
  ## Enable creation of NetworkPolicy resources.
  ##
  enabled: false
